{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2\n",
    "## Exercise 1 + 2 + 3\n",
    "First we adding some imports that we are going to use define the provided functions of `mnist-helper.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import numpy as np\n",
    "import gzip\n",
    "from gzip import GzipFile\n",
    "# make all of ths reproducible\n",
    "np.random.seed(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import time\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "    \n",
    "from math import sqrt\n",
    "from tensorflow import truncated_normal_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    '''``MNISTLoader`` class can load MNIST dataset from the web or disk.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    training_data_name  :   str\n",
    "                            Name of the train data file\n",
    "    test_data_name  :   str\n",
    "                        Name of the test data file\n",
    "    training_labels_name    :   str\n",
    "                                Name of the train labels file\n",
    "    test_labels_name    :   str\n",
    "                            Name of the test labels file\n",
    "    data_folder :   str\n",
    "                    Name of the folder to save data in\n",
    "    training_data   :   np.ndarray\n",
    "                        Numpy array with the training data\n",
    "    test_data   :   np.ndarray\n",
    "                    Numpy array with the test data\n",
    "    training_labels :   np.ndarray\n",
    "                        Training labels\n",
    "    test_labels :   np.ndarray\n",
    "                    Test labels\n",
    "    '''\n",
    "    def __init__(self, directory='data', base_link='http://yann.lecun.com/exdb/mnist/'):\n",
    "        '''Initialise loader.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory   :   str\n",
    "                        Directory to save downloaded data to\n",
    "        base_link   :   str\n",
    "                        Uri forming the base for all data parts. File names will\n",
    "                        be appended to this base\n",
    "        '''\n",
    "        self.training_data_name = 'train-images-idx3-ubyte.gz'\n",
    "        self.training_labels_name = 'train-labels-idx1-ubyte.gz'\n",
    "        self.test_data_name = 't10k-images-idx3-ubyte.gz'\n",
    "        self.test_labels_name = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "        self.data_folder = directory\n",
    "\n",
    "        # if folder doesn't exist yet, create it and download\n",
    "        if not os.path.exists(self.data_folder):\n",
    "            os.mkdir(self.data_folder)\n",
    "            # helper to shorten url creation\n",
    "            url = lambda name: urljoin(base_link, name)\n",
    "            self.test_data = self._load(url(self.test_data_name), False, True)\n",
    "            self.test_labels = self._load(url(self.test_labels_name),\n",
    "                                          True, True)\n",
    "            self.training_data = self._load(url(self.training_data_name),\n",
    "                                            False, True)\n",
    "            self.training_labels = self._load(url(self.training_labels_name),\n",
    "                                              True, True)\n",
    "        # else simply load from disk\n",
    "        else:\n",
    "            # helper to shorten path creation\n",
    "            path = lambda name: os.path.join(directory, name)\n",
    "            self.test_data = self._load(path(self.test_data_name))\n",
    "            self.test_labels = self._load(path(self.test_labels_name), True)\n",
    "            self.training_data = self._load(path(self.training_data_name))\n",
    "            self.training_labels = self._load(path(self.training_labels_name), True)\n",
    "\n",
    "    def _load(self, path_or_url, labels=False, save=False):\n",
    "        '''Load the MNIST data set, either from disk or from the web.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_url :   str\n",
    "                        Url of the file, or filesystem path\n",
    "        labels  :   bool\n",
    "                    Whether or not the file contains labels as opposed to data\n",
    "        save    :   bool\n",
    "                    Serialize the data to disk (useless if fetching locally)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "                Data (in shape (N, 28, 28)) or labels (in shape (N))\n",
    "        '''\n",
    "\n",
    "        # we unify loading from web and file system by creating file:// uris for\n",
    "        # local files and just using urllib on them as well\n",
    "        parse_result = urlparse(path_or_url)\n",
    "        if not parse_result.scheme:\n",
    "            # looks like a file\n",
    "            if not os.path.exists(path_or_url):\n",
    "                raise RuntimeError('Found data directory, but not %s in it. '\n",
    "                                   'Remove it and restart' % path_or_url)\n",
    "            path_or_url = 'file://' + os.path.abspath(path_or_url)\n",
    "        else:\n",
    "            print('Downloading from web...')\n",
    "        print(path_or_url)\n",
    "        # stream over tcp/file\n",
    "        with urlopen(path_or_url) as request_stream:\n",
    "            zip_file = GzipFile(fileobj=request_stream, mode='rb')\n",
    "            zip_name = os.path.join(self.data_folder, os.path.basename(path_or_url))\n",
    "            if save:\n",
    "                # first save the file\n",
    "                with gzip.open(zip_name, mode='wb') as f:\n",
    "                    f.write(zip_file.read())\n",
    "            # then read it back in and fill data\n",
    "            # note we cannot simply seek(0) above since this isn't a real file\n",
    "            # but a web resource\n",
    "            with gzip.open(zip_name, mode='rb') as fd:\n",
    "                # first unpack magic numer and number of elements (4 bytes each)\n",
    "                magic, numberOfItems = struct.unpack('>ii', fd.read(2 * 4))\n",
    "                if (not labels and magic != 2051) or (labels and magic != 2049):\n",
    "                    raise LookupError('Not a MNIST file')\n",
    "                if not labels:\n",
    "                    # then unpack format\n",
    "                    rows, cols = struct.unpack('>II', fd.read(8))\n",
    "                    # to use np.frombuffer, we need a bytearray, doesn't work\n",
    "                    # directly from file\n",
    "                    b = bytearray(fd.read())\n",
    "                    images = np.frombuffer(b, dtype='uint8')\n",
    "                    images = images.reshape((numberOfItems, rows, cols))\n",
    "                    return images\n",
    "                else:\n",
    "                    b = bytearray(fd.read())\n",
    "                    labels = np.frombuffer(b, dtype='uint8')\n",
    "                    return labels\n",
    "\n",
    "    def batches(self, data, labels, batch_size):\n",
    "        '''Generate a set of random minibatches from the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data    :   np.ndarray\n",
    "                    Data array\n",
    "        lablels :   np.ndarray\n",
    "                    Labels vectory\n",
    "        batch_size  :   int\n",
    "                        Size of the minibatches (must be > 0)\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        tuple\n",
    "            Tuples of (data_batch, labels_batch), the union of which exactly\n",
    "            equals the data set\n",
    "        '''\n",
    "        samples_n = labels.shape[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = samples_n\n",
    "\n",
    "        random_indices = np.random.choice(samples_n, samples_n, replace=False)\n",
    "        data = data[random_indices]\n",
    "        labels = labels[random_indices]\n",
    "        for i in range(samples_n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "            yield data[on:off], labels[on:off]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Here we are inspecting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_digits(*digits_labels):\n",
    "    '''Plot an aribtrary number of mnis digits on an automatic grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    digits_labels   :   list\n",
    "                        List of tuples (np.ndarray, int) giving an image of\n",
    "                        shape (28, 28) and a label\n",
    "    Returns\n",
    "    -------\n",
    "    pyplot.Figure\n",
    "            Figure with digits drawn into the only axes\n",
    "    '''\n",
    "    num = len(digits_labels)\n",
    "    # try a square shape, but round to the nearest integer (down for rows, up\n",
    "    # for cols)\n",
    "    rows = int(sqrt(num))\n",
    "    cols = int(num / rows + 0.5)\n",
    "    f, axarr = plt.subplots(rows, cols)\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            ax = axarr[row][col]\n",
    "            # linear index from two indeces\n",
    "            index = row * rows + col\n",
    "            ax.imshow(digits_labels[index][0], cmap='gray')\n",
    "            # place class label to the left of plot\n",
    "            ax.set_title(digits_labels[index][1], x=-0.1, y=0.5)\n",
    "            # remove the ticks (pointless for images)\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Create the data flow graph (DFG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph in order to cope with multiple cell executions.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Weight matrix\n",
    "mean = 0.0\n",
    "std = 0.000002\n",
    "W = tf.get_variable('weights', initializer=truncated_normal_initializer(mean, std, seed=1), shape=[28 * 28, 10])\n",
    "\n",
    "# bias vector\n",
    "b = tf.get_variable('bias', initializer=tf.zeros_initializer(), shape=[10])\n",
    "\n",
    "# data vector\n",
    "x = tf.placeholder(tf.float32, [None, 28 * 28], name='input')\n",
    "\n",
    "# desired output (ie real labels)\n",
    "d = tf.placeholder(tf.int32, [None, 1], name='labels')\n",
    "# one-hot encoding produces a vecor of shape (batch, 1, 10) instead of (batch, 10)\n",
    "d_1_hot = tf.squeeze(tf.one_hot(d, 10), axis=1)\n",
    "\n",
    "# computed output of the network without activation\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# loss function\n",
    "cross_entropy      = tf.nn.softmax_cross_entropy_with_logits(logits = y, labels = d_1_hot)\n",
    "mean_cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "# The optimizer should minimize the cross_entropy.\n",
    "optimizer          = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)#0.5)\n",
    "training_step      = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 + 7\n",
    "Now we are training the network and store all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "loader = MNISTLoader()\n",
    "d_train, l_train, d_test, l_test = (loader.training_data,\n",
    "                                    loader.training_labels,\n",
    "                                    loader.test_data, loader.test_labels)\n",
    "\n",
    "# The data comes in image format, which we flatten\n",
    "d_test  = np.reshape(d_test, (-1, 28 * 28))\n",
    "d_train = np.reshape(d_train, (-1, 28 * 28))\n",
    "\n",
    "# The labels only have 1 dimensions, we need to blow it up to 2\n",
    "l_test  = l_test[:, np.newaxis]\n",
    "l_train = l_train[:, np.newaxis]\n",
    "\n",
    "# check if neuron firing strongest coincides with max value position in real labels\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(d_1_hot, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# record accuracy\n",
    "training_step_accuracy = []\n",
    "test_step_accuracy     = []\n",
    "\n",
    "# record cross-entropy\n",
    "training_step_entropy = []\n",
    "test_step_entropy     = []\n",
    "\n",
    "# record weights\n",
    "weights = []\n",
    "\n",
    "# Specify number of epochs and the batch size\n",
    "n_epochs   = 3\n",
    "batch_size = 5000\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    i = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch %d' % epoch)\n",
    "        for mb, labels in loader.batches(d_train, l_train, batch_size):\n",
    "            # pass a dict to later retrieve tensors by name. We need the\n",
    "            # weights only for the record\n",
    "            values = sess.run(\n",
    "                {'weights': W, 'step': training_step},\n",
    "                feed_dict={x: mb, d: labels})\n",
    "            if i % 10 == 0:\n",
    "                # run the ops that will give us accuracies and entropies\n",
    "                # (not needed otherwise)\n",
    "                current_train_accuracy = sess.run(\n",
    "                    accuracy, feed_dict={x: d_train, d: l_train})\n",
    "                current_test_accuracy = sess.run(\n",
    "                    accuracy, feed_dict={x: d_test, d: l_test})\n",
    "\n",
    "                training_step_accuracy.append(current_train_accuracy)\n",
    "                test_step_accuracy.append(current_test_accuracy)\n",
    "\n",
    "                current_train_entropy = sess.run(\n",
    "                    mean_cross_entropy, feed_dict={\n",
    "                        x: d_train, d: l_train})\n",
    "                current_test_entropy = sess.run(\n",
    "                    mean_cross_entropy, feed_dict={\n",
    "                        x: d_test, d: l_test})\n",
    "\n",
    "                training_step_entropy.append(current_train_entropy)\n",
    "                test_step_entropy.append(current_test_entropy)\n",
    "\n",
    "                weights.append(np.reshape(values['weights'], (28, 28, 10)))\n",
    "\n",
    "            # increment batch counter\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Monitor the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Problem: We append the accuray every 10th step, so we may miss the last one\n",
    "print('(Almost) final test accuracy: %f' % test_step_accuracy[-1])\n",
    "\n",
    "# Plot entropy and accuray\n",
    "f = plt.figure()\n",
    "ax_acc = f.add_subplot(121)\n",
    "ax_acc.set_title('Accuracy over training and test sets')\n",
    "ax_acc.set_xlabel('(n*10)th batch')\n",
    "ax_acc.set_ylabel('Accuracy')\n",
    "ax_acc.plot(test_step_accuracy, linestyle=':', label='Test set')\n",
    "ax_acc.plot(training_step_accuracy, linestyle=':', label='Training set')\n",
    "ax_acc.legend()\n",
    "\n",
    "ax_entropy = f.add_subplot(122)\n",
    "ax_entropy.set_title('Cross Entropy over training and test sets')\n",
    "ax_entropy.set_xlabel('(n*10)th batch')\n",
    "ax_entropy.set_ylabel('Cross Entropy')\n",
    "ax_entropy.plot(test_step_entropy, linestyle=':', label='Test set')\n",
    "ax_entropy.plot(training_step_entropy, linestyle=':', label='Training set')\n",
    "ax_entropy.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot weights interactively\n",
    "rows, cols = (2, 5)\n",
    "f2, axarr = plt.subplots(rows, cols)\n",
    "plt.ion()\n",
    "for row in range(2):\n",
    "    for col in range(5):\n",
    "        ax = axarr[row][col]\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    current_weights = weights[i]\n",
    "    for row in range(2):\n",
    "        for col in range(5):\n",
    "            f2.suptitle('Step %d of %d' % (i, len(weights)))\n",
    "            ax = axarr[row][col]\n",
    "            ax.cla()\n",
    "            index = row * rows + col\n",
    "            ax.set_title('Neuron %d' % index)\n",
    "            # there's many diverging cmaps\n",
    "            # (https://matplotlib.org/examples/color/colormaps_reference.html)\n",
    "            ax.imshow(current_weights[..., index], cmap='Spectral')\n",
    "    # pause so that it always takes 5 seconds\n",
    "    # Note: The animation seems to slow down linearly, unless we clear the\n",
    "    # axes (see above).\n",
    "    # Uncomment this function if you have pyplot > v2.11. Details:\n",
    "    #     https://stackoverflow.com/questions/46982150/matplotlib-pyplot-global-name-time-is-not-defined-error\n",
    "    # also, this doesn't seem to run in notebooks. Run the accompanying exercise5.py file instead.\n",
    "    #       plt.pause(5 / len(weights)) "
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}