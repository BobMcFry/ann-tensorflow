{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple, DropoutWrapper\n",
    "from tensorflow import (placeholder, cond, reduce_mean, reduce_sum, where, not_equal, ones_like,\n",
    "                        zeros_like, reshape, equal, constant, cast, concat, argmax, Variable)\n",
    "from tensorflow import nn\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import os\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "weights_n = 0\n",
    "\n",
    "def get_weights_and_bias(shape, shape_b=None, dtype=tf.float32,\n",
    "        initializer_w=tf.random_normal_initializer(),\n",
    "        initializer_b=tf.zeros_initializer()):\n",
    "    if not shape_b:\n",
    "        shape_b = shape[-1:]\n",
    "\n",
    "    global weights_n\n",
    "\n",
    "    weights_n += 1\n",
    "    with tf.variable_scope('weights%d' % weights_n):\n",
    "        return (\n",
    "                tf.get_variable('W', initializer=initializer_w,\n",
    "                                shape=shape, dtype=dtype),\n",
    "                tf.get_variable('b', shape=shape_b, initializer=initializer_b)\n",
    "                )\n",
    "\n",
    "def get_optimizer(name):\n",
    "    if isinstance(name, tf.train.Optimizer):\n",
    "        return name\n",
    "    else:\n",
    "        return getattr(tf.train, name + 'Optimizer')\n",
    "\n",
    "fc_n = 0\n",
    "\n",
    "\n",
    "def fully_connected(input, n_out, with_activation=False, activation=tf.nn.tanh,\n",
    "        use_bias=True):\n",
    "    '''Create a fully connected layer with fixed activation function and variable\n",
    "    initialisation. The activation function is ``tf.nn.tanh`` and variables are\n",
    "    initialised from a truncated normal distribution with an stddev of 0.1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input   :   tf.Variable\n",
    "                Input to the layer\n",
    "    n_out   :   int\n",
    "                Number of neurons in the layer\n",
    "    with_activation :   bool\n",
    "                        Return activation or drive (useful when planning to use\n",
    "                        ``softmax_cross_entropy_with_logits`` which requires\n",
    "                        unscaled logits)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "            The variable representing the layer activation (tanh(input * Weights\n",
    "            + bias))\n",
    "    '''\n",
    "    global fc_n\n",
    "    fc_n += 1\n",
    "    with tf.variable_scope('fully%d' % fc_n):\n",
    "        (fan_in, fan_out) = (input.shape[-1].value, n_out)\n",
    "        if activation == tf.nn.tanh:\n",
    "            init_W = tf.random_normal_initializer(stddev=fan_in ** (-0.5))\n",
    "        elif activation == tf.nn.relu:\n",
    "            init_W = tf.random_normal_initializer(stddev=2 / fan_in)\n",
    "        else:\n",
    "            init_W = tf.random_normal_initializer()\n",
    "        init_b = tf.constant_initializer(0.1)\n",
    "        W = tf.get_variable(\n",
    "                'weights',\n",
    "                initializer=init_W,\n",
    "                shape=(input.shape[-1], n_out), # the last dim of the input\n",
    "               dtype=tf.float32                 # is the 1st dim of the weights\n",
    "            )\n",
    "        if use_bias:\n",
    "            bias = tf.get_variable('bias', initializer=init_b, shape=(n_out,))\n",
    "        if use_bias:\n",
    "            drive = tf.matmul(input, W) + bias\n",
    "        else:\n",
    "            drive = tf.matmul(input, W)\n",
    "        if with_activation:\n",
    "            return activation(drive)\n",
    "        else:\n",
    "            return drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_NAME = 'imdb_helper.pckl'\n",
    "\n",
    "class IMDB:\n",
    "\n",
    "    def load(self):\n",
    "        '''Deserialise self from pickeld file.'''\n",
    "        f = open(PICKLE_NAME, 'rb')\n",
    "        tmp_dict = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.__dict__.update(tmp_dict)\n",
    "\n",
    "    def save(self):\n",
    "        '''Serialise self to pickeld file.'''\n",
    "        f = open(PICKLE_NAME, 'wb')\n",
    "        pickle.dump(self.__dict__, f, 2)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def __init__(self, directory):\n",
    "        if os.path.exists(PICKLE_NAME):\n",
    "            self.load()\n",
    "        else:\n",
    "            self._directory = directory\n",
    "\n",
    "            self._training_data, self._training_labels = self._load_data('train')\n",
    "            self._test_data, self._test_labels = self._load_data('test')\n",
    "\n",
    "            np.random.seed(0)\n",
    "            samples_n = self._training_labels.shape[0]\n",
    "            random_indices = np.random.choice(samples_n, samples_n // 7, replace = False)\n",
    "            np.random.seed()\n",
    "\n",
    "            self._validation_data = self._training_data[random_indices]\n",
    "            self._validation_labels = self._training_labels[random_indices]\n",
    "            self._training_data = np.delete(self._training_data, random_indices, axis = 0)\n",
    "            self._training_labels = np.delete(self._training_labels, random_indices)\n",
    "\n",
    "            joined_written_ratings = [word for text in self._training_data for word in text]\n",
    "            print('Unique words: ' + str(len(Counter(joined_written_ratings))))\n",
    "            print('Mean length: ' + str(np.mean([len(text) for text in self._training_data])))\n",
    "            self.save()\n",
    "\n",
    "\n",
    "    def _load_data(self, data_set_type):\n",
    "        data = []\n",
    "        labels = []\n",
    "        # Iterate over conditions\n",
    "        for condition in ['neg', 'pos']:\n",
    "            directory_str = os.path.join(self._directory, 'aclImdb', data_set_type, condition)\n",
    "            directory = os.fsencode(directory_str)\n",
    "\n",
    "            for file in os.listdir(directory):\n",
    "                filename = os.fsdecode(file)\n",
    "\n",
    "                label = 0 if condition == 'neg' else 1\n",
    "                labels.append(label)\n",
    "\n",
    "                # Read written rating from file\n",
    "                with open(os.path.join(directory_str, filename)) as fd:\n",
    "                    written_rating = fd.read()\n",
    "                    written_rating = written_rating.lower()\n",
    "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                    written_rating = tokenizer.tokenize(written_rating)\n",
    "                    data.append(written_rating)\n",
    "\n",
    "        return np.array(data), np.array(labels)\n",
    "\n",
    "    def create_dictionaries(self, vocabulary_size, cutoff_length):\n",
    "        if not hasattr(self, '_word2id'):\n",
    "            joined_written_ratings = [word for text in self._training_data for word in text]\n",
    "            words_and_count = Counter(joined_written_ratings).most_common(vocabulary_size - 2)\n",
    "\n",
    "            word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 2)}\n",
    "            word2id['_UNKNOWN_'] = 0\n",
    "            word2id['_NOT_A_WORD_'] = 1\n",
    "\n",
    "            id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "            self._word2id = word2id\n",
    "            self._id2word = id2word\n",
    "\n",
    "            self._training_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._training_data])\n",
    "            self._validation_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._validation_data])\n",
    "            self._test_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._test_data])\n",
    "            self.save()\n",
    "\n",
    "\n",
    "    def words2ids(self, words):\n",
    "        if type(words) == list or type(words) == range or type(words) == np.ndarray:\n",
    "            return [self._word2id.get(word, 0) for word in words]\n",
    "        else:\n",
    "            return self._word2id.get(words, 0)\n",
    "\n",
    "    def ids2words(self, ids):\n",
    "        if type(ids) == list or type(ids) == range or type(ids) == np.ndarray:\n",
    "            return [self._id2word.get(wordid, '_UNKNOWN_') for wordid in ids]\n",
    "        else:\n",
    "            return self._id2word.get(ids, '_UNKNOWN_')\n",
    "\n",
    "\n",
    "    def get_training_batch(self, batch_size):\n",
    "        return self._get_batch(self._training_data, self._training_labels, batch_size)\n",
    "\n",
    "    def get_validation_batch(self, batch_size):\n",
    "        return self._get_batch(self._validation_data, self._validation_labels, batch_size)\n",
    "\n",
    "    def get_test_batch(self, batch_size):\n",
    "        return self._get_batch(self._test_data, self._test_labels, batch_size)\n",
    "\n",
    "    def _get_batch(self, data, labels, batch_size):\n",
    "        samples_n = labels.shape[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = samples_n\n",
    "\n",
    "        random_indices = np.random.choice(samples_n, samples_n, replace = False)\n",
    "        data = data[random_indices]\n",
    "        labels = labels[random_indices]\n",
    "\n",
    "        for i in range(samples_n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "            yield data[on:off], labels[on:off]\n",
    "\n",
    "\n",
    "    def slice_batch(self, batch, slice_size):\n",
    "        max_len = np.max([len(sample) for sample in batch])\n",
    "        steps = int(np.ceil(max_len / slice_size))\n",
    "        max_len = slice_size * steps\n",
    "\n",
    "        # Resize all samples in batch to same size\n",
    "        batch_size = len(batch)\n",
    "        # fill buffer with _NOT_A_WORD_\n",
    "        buffer = np.ones((batch_size, max_len), dtype = np.int32)\n",
    "        for i, sample in enumerate(batch):\n",
    "            buffer[i, 0:len(sample)] = sample\n",
    "\n",
    "        for i in range(steps):\n",
    "            on = i * slice_size\n",
    "            off = on + slice_size\n",
    "            yield buffer[:, on:off]\n",
    "\n",
    "\n",
    "    def get_sizes(self):\n",
    "        training_samples_n   = self._training_labels.shape[0]\n",
    "        validation_samples_n = self._validation_labels.shape[0]\n",
    "        test_samples_n       = self._test_labels.shape[0]\n",
    "        return training_samples_n, validation_samples_n, test_samples_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Optimizer Helper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerSpec(dict):\n",
    "    '''Encapsulate all the info needed for creating any kind of optimizer. Learning rate scheduling\n",
    "    is fixed to exponential decay\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    step_counter    :   Variable\n",
    "                        Counter to be passed to optimizer#minimize() so it gets incremented during\n",
    "                        each update\n",
    "    learning_rate   :   tf.train.piecewise_constant\n",
    "                        Learning rate of the optimizer (for later retrieval)\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        kind    :   str\n",
    "                    Name of the optimizer\n",
    "        learning_rate   :   float\n",
    "                            Base learning rate used\n",
    "        name    :   str\n",
    "                    Optional name for the piecewise_constant operation\n",
    "        momentum    :   float\n",
    "                        Optional momentum for momentum optimizers\n",
    "        use_nesterov    :   bool\n",
    "                            Nesterov flag for momentum optimizer\n",
    "        '''\n",
    "        if not 'kind' in kwargs:\n",
    "            raise ValueError('No optimizer name given')\n",
    "        if not 'learning_rate' in kwargs:\n",
    "            raise ValueError('No base learning_rate given')\n",
    "        self.update(kwargs)\n",
    "        self.step_counter  = Variable(0, trainable=False, dtype=tf.int32, name='step_counter')\n",
    "        rate               = kwargs['learning_rate']\n",
    "        steps              = kwargs.get('steps', 100)\n",
    "        decay              = kwargs.get('decay', 0.8)\n",
    "        self.learning_rate = tf.train.exponential_decay(rate, self.step_counter, steps, decay)\n",
    "\n",
    "    def create(self):\n",
    "        '''Build the Optimizer object from the properties\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        tf.train.Optimizer\n",
    "            Ready-made optimizer\n",
    "        '''\n",
    "        kind          = self['kind']\n",
    "        learning_rate = self.learning_rate\n",
    "        name          = self.get('name', 'optimizer')\n",
    "        optimizer_cls = get_optimizer(kind)\n",
    "        if kind in ['Momentum', 'RMSProp']:\n",
    "            # only those two use momentum param\n",
    "            try:\n",
    "                momentum = self['momentum']\n",
    "            except KeyError:\n",
    "                raise ValueError('Momentum parameter is necessary for MomentumOptimizer')\n",
    "            if kind == 'Momentum':\n",
    "                if 'use_nesterov' in self:\n",
    "                    use_nesterov = self['use_nesterov']\n",
    "                else:\n",
    "                    use_nesterov = False\n",
    "                return optimizer_cls(learning_rate, momentum, use_nesterov, name=name)\n",
    "            else:\n",
    "                return optimizer_cls(learning_rate, momentum, name=name)\n",
    "        else:\n",
    "            return optimizer_cls(learning_rate, name=name)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        key_val_str = ', '.join(str(k) + '=' + str(v) for k, v in self.items())\n",
    "        return f'<Optimizer: {key_val_str}>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBModel(object):\n",
    "    '''Model for IMBD movie review classification.'''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        '''The following arguments are accepted:\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size  :   int\n",
    "                        Size of the vocabulary for creating embeddings\n",
    "        embedding_matrix    :   int\n",
    "                                Dimensionality of the embedding space\n",
    "        memory_size :   int\n",
    "                        LSTM memory size\n",
    "        keep_prob   :   Inverse of dropout percentage for embedding and LSTM\n",
    "        subsequence_length  :   Length of the subsequences (all embeddings are padded to this length)\n",
    "        optimizer   :   OptimizerSpec\n",
    "\n",
    "        '''\n",
    "        ############################################################################################\n",
    "        #                                 Get all hyperparameters                                  #\n",
    "        ############################################################################################\n",
    "        vocab_size         = kwargs['vocab_size']\n",
    "        embedding_size     = kwargs['embedding_size']\n",
    "        memory_size        = kwargs['memory_size']\n",
    "        keep_prob          = kwargs['keep_prob']\n",
    "        subsequence_length = kwargs['subsequence_length']\n",
    "        optimizer_spec     = kwargs['optimizer']\n",
    "        optimizer          = optimizer_spec.create()\n",
    "        self.learning_rate = optimizer_spec.learning_rate\n",
    "        self.step_counter  = optimizer_spec.step_counter\n",
    "\n",
    "        ############################################################################################\n",
    "        #                                        Net inputs                                        #\n",
    "        ############################################################################################\n",
    "        self.batch_size   = placeholder(tf.int32,   shape=[],                  name='batch_size')\n",
    "        self.is_training  = placeholder(tf.bool,    shape=[],                  name='is_training')\n",
    "        self.word_ids     = placeholder(tf.int32,   shape=(None, subsequence_length),\n",
    "                                                                               name='word_ids')\n",
    "        self.labels       = placeholder(tf.int32,   shape=(None,),             name='labels')\n",
    "        self.hidden_state = placeholder(tf.float32, shape=(None, memory_size), name='hidden_state')\n",
    "        self.cell_state   = placeholder(tf.float32, shape=(None, memory_size), name='cell_state')\n",
    "\n",
    "        lengths = sequence_lengths(self.word_ids)\n",
    "\n",
    "        ############################################################################################\n",
    "        #                                        Embedding                                         #\n",
    "        ############################################################################################\n",
    "        self.embedding_matrix, _bias = get_weights_and_bias((vocab_size, embedding_size))\n",
    "        embeddings = cond(self.is_training,\n",
    "                         lambda: nn.dropout(\n",
    "                             nn.embedding_lookup(self.embedding_matrix, self.word_ids),\n",
    "                             keep_prob=keep_prob),\n",
    "                         lambda: nn.embedding_lookup(self.embedding_matrix, self.word_ids)\n",
    "                         )\n",
    "\n",
    "        ############################################################################################\n",
    "        #                                        LSTM layer                                        #\n",
    "        ############################################################################################\n",
    "        cell = BasicLSTMCell(memory_size, activation=tf.nn.tanh)\n",
    "\n",
    "        # during inference, use entire ensemble\n",
    "        keep_prob = cond(self.is_training, lambda: constant(keep_prob), lambda: constant(1.0))\n",
    "        cell      = DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "        # what's the difference to just creating a zero-filled tensor tuple?\n",
    "        self.zero_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "        state           = LSTMStateTuple(c=self.cell_state, h=self.hidden_state)\n",
    "\n",
    "        # A dynamic rnn creates the graph on the fly, so it can deal with embeddings of different\n",
    "        # lengths. We do not need to unstack the embedding tensor to get rows, instead we compute\n",
    "        # the actual sequence lengths and pass that\n",
    "        # We are not sure how any of this works. Do we need to mask the cost function so the cell\n",
    "        # outputs for _NOT_A_WORD_ inputs are ignored? Is the final cell state really relevant if it\n",
    "        # was last updated with _NOT_A_WORD_ input? Does static_rnn absolve us of any of those\n",
    "        # issues?\n",
    "        outputs, self.state = nn.dynamic_rnn(cell, embeddings, sequence_length=lengths,\n",
    "                                             initial_state=state)\n",
    "        # Recreate tensor from list\n",
    "        outputs      = reshape(concat(outputs, 1), [-1, subsequence_length * memory_size])\n",
    "        self.outputs = reduce_mean(outputs)\n",
    "\n",
    "        ############################################################################################\n",
    "        #                        Fully connected layer, loss, and training                         #\n",
    "        ############################################################################################\n",
    "        ff1  = fully_connected(outputs, 2, with_activation=False, use_bias=True)\n",
    "        loss = reduce_mean(nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels,\n",
    "                                                                            logits=ff1))\n",
    "        self.train_step    = optimizer.minimize(loss, global_step=self.step_counter)\n",
    "        self.predictions   = nn.softmax(ff1)\n",
    "        correct_prediction = equal(cast(argmax(self.predictions, 1), tf.int32), self.labels)\n",
    "        self.accuracy      = reduce_mean(cast(correct_prediction, tf.float32))\n",
    "\n",
    "        ############################################################################################\n",
    "        #                                    Create summaraies                                     #\n",
    "        ############################################################################################\n",
    "        with tf.variable_scope('summary'):\n",
    "            self.summary_loss = tf.summary.scalar('loss', loss)\n",
    "            self.summary_accuracy = tf.summary.scalar('accuracy', self.accuracy)\n",
    "\n",
    "\n",
    "    def get_zero_state(self, session, batch_size):\n",
    "        '''Retrieve the LSTM zero state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        session :   tf.Session\n",
    "                    Open session to run the op in\n",
    "        batch_size  :   int\n",
    "                        Batch size (required for the tensor shapes, since the state cannot have\n",
    "                        variable dimensions)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        LSTMStateTuple\n",
    "            Tuple of zero tensors of shape [batch_size x memory_size]\n",
    "        '''\n",
    "        return session.run(self.zero_state, feed_dict = {self.batch_size: batch_size})\n",
    "\n",
    "    def run_training_step(self, session, subsequence_batch, labels, state):\n",
    "        '''Run one training step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        session :   tf.Session\n",
    "                    Open session to run ops in\n",
    "        subsequence_batch   :   np.ndarray\n",
    "                                Array of subsequences\n",
    "        labels  :   np.ndarray\n",
    "                    Array of labels for each batch\n",
    "        state   :   LSTMStateTuple\n",
    "                    LSTM memory state from the last step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        LSTMStateTuple, Tensor\n",
    "            New memory state and the summary tensor for the loss op\n",
    "\n",
    "        '''\n",
    "        state, _, summary_loss = session.run([self.state, self.train_step, self.summary_loss],\n",
    "            feed_dict = {\n",
    "                self.word_ids:     subsequence_batch,\n",
    "                self.labels:       labels,\n",
    "                self.cell_state:   state.c,\n",
    "                self.hidden_state: state.h,\n",
    "                self.batch_size:   subsequence_batch.shape[0],\n",
    "                self.is_training:  True\n",
    "            })\n",
    "        return state, summary_loss\n",
    "\n",
    "    def run_test_step(self, session, subsequence_batch, labels):\n",
    "        '''Run one test step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        session :   tf.Session\n",
    "                    Open session to run ops in\n",
    "        subsequence_batch   :   np.ndarray\n",
    "                                Array of subsequences\n",
    "        labels  :   np.ndarray\n",
    "                    Array of labels for each batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float, Tensor\n",
    "            Accuracy and the summary tensor for the accuracy on the batch\n",
    "\n",
    "        '''\n",
    "        batch_size = subsequence_batch.shape[0]\n",
    "        zero_state = self.get_zero_state(session, batch_size)\n",
    "        predictions, accuracy, summary_accuracy = session.run([self.predictions, self.accuracy, self.summary_accuracy],\n",
    "            feed_dict = {\n",
    "                self.word_ids:     subsequence_batch,\n",
    "                self.labels:       labels,\n",
    "                self.cell_state:   zero_state.c,\n",
    "                self.hidden_state: zero_state.h,\n",
    "                self.batch_size:   batch_size,\n",
    "                self.is_training:  False\n",
    "            })\n",
    "        return accuracy, summary_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "sequence_length = 100   # Length for subsequence training.\n",
    "cutoff = 300            # Cutoff length for reviews.\n",
    "batch_size = 250        # Batch size\n",
    "epochs = 2              # Number of epochs'\n",
    "learning_rate = 0.03    # Initial learning rate (scheduling is used)\n",
    "embedding_size = 64     # Embedding dimensionality'\n",
    "memory_size = 64        # Memory size'\n",
    "keep_probability = 0.85 # Percentage of neurons to keep during dropout'\n",
    "momentum = 0.5          # Momentum (only used for Momentum optimizer)'\n",
    "optimizer = 'Adam'      # Optimizer class'\n",
    "decay_steps = 100       # Decay learning rate every n steps'\n",
    "decay_rate = 0.8        # Base decay value for exponential decay'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB data\n",
      "Unique words: 70610\n",
      "Mean length: 242.911288441\n",
      "Using optimizer <Optimizer: learning_rate=0.03, steps=100, decay=0.8, kind=Adam, momentum=0.5, use_nesterov=True>\n",
      "Probable number of steps: 516\n",
      "Creating model\n",
      "Starting epoch 0\n",
      "Accuracy = 0.489\n",
      "Accuracy = 0.565\n",
      "Accuracy = 0.644\n",
      "Accuracy = 0.691\n",
      "Accuracy = 0.721\n",
      "Accuracy = 0.765\n",
      "Accuracy = 0.776\n",
      "Accuracy = 0.778\n",
      "Accuracy = 0.790\n",
      "Starting epoch 1\n",
      "Accuracy = 0.795\n",
      "Accuracy = 0.791\n",
      "Accuracy = 0.796\n",
      "Accuracy = 0.799\n",
      "Accuracy = 0.794\n",
      "Accuracy = 0.792\n",
      "Accuracy = 0.800\n",
      "Accuracy = 0.809\n",
      "Accuracy = 0.795\n"
     ]
    }
   ],
   "source": [
    "def sequence_lengths(sequences, padding_value=1):\n",
    "    '''Find the actual sequence length for each sequence in a tensor. Sequences could be padded with\n",
    "    1s if they were shorter than the cutoff length chosen.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequences   :   tf.Tensor\n",
    "                    Tensor of shape [batch_size x sequence_length]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        Tensor of shape [batch_size,], each value being the true length of its associated sequence\n",
    "    '''\n",
    "    _1 = tf.fill(tf.shape(sequences), padding_value)\n",
    "    _0 = zeros_like(sequences)\n",
    "    # set values != 1 to 1 and the rest to 0, so the sum is the number\n",
    "    # of nonzeros\n",
    "    is_padding = where(not_equal(sequences, _1), _1, _0)\n",
    "    return reduce_sum(is_padding, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def estimate_number_of_steps(train_data, sequence_length, epochs, batch_size):\n",
    "    '''Get an (incorrect, but close) estimate for the number of training steps. This is useful for\n",
    "    choosing a learning rate schedule.'''\n",
    "\n",
    "    batches = int(train_data.shape[0] / batch_size + 0.5)\n",
    "    max_len = np.max([len(sample) for sample in train_data])\n",
    "    steps   = int(np.ceil(max_len / sequence_length)) * batches * epochs\n",
    "    return steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "#                                        Load the data                                         #\n",
    "################################################################################################\n",
    "print('Loading IMDB data')\n",
    "helper = IMDB('data')\n",
    "helper.create_dictionaries(vocabulary_size, cutoff)\n",
    "\n",
    "opti_spec = OptimizerSpec(learning_rate=learning_rate, steps=decay_steps,\n",
    "                          decay=decay_rate, kind=optimizer, momentum=momentum,\n",
    "                          use_nesterov=True)\n",
    "print(f'Using optimizer {opti_spec}')\n",
    "steps      = estimate_number_of_steps(helper._training_data, sequence_length, epochs, batch_size)\n",
    "print(f'Probable number of steps: {steps}')\n",
    "\n",
    "################################################################################################\n",
    "#                                     Initialise the model                                     #\n",
    "################################################################################################\n",
    "print('Creating model')\n",
    "model = IMDBModel(vocab_size=vocabulary_size,\n",
    "                  subsequence_length=sequence_length,\n",
    "                  optimizer=opti_spec,\n",
    "                  embedding_size=embedding_size,\n",
    "                  memory_size=memory_size,\n",
    "                  keep_prob=keep_probability)\n",
    "\n",
    "summary_dir = './summary/train/'\n",
    "\n",
    "################################################################################################\n",
    "#                                       Run all the shit                                       #\n",
    "################################################################################################\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    counter = 1\n",
    "    train_writer = tf.summary.FileWriter(summary_dir, session.graph)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Starting epoch {epoch}')\n",
    "\n",
    "        for batch_idx, (batch, labels) in enumerate(helper.get_training_batch(batch_size)):\n",
    "            # reset state for each batch\n",
    "            state = model.get_zero_state(session, batch_size)\n",
    "\n",
    "            for subsequence_batch in helper.slice_batch(batch, sequence_length):\n",
    "                # push one subsequence of each batch member\n",
    "                state, summary_loss = model.run_training_step(session, subsequence_batch, labels, state)\n",
    "                if counter % 10 == 0:\n",
    "                    train_writer.add_summary(summary_loss, counter)\n",
    "                counter += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                ###############################\n",
    "                #  Test with 5000 test data.  #\n",
    "                ###############################\n",
    "                samples_n                  = helper._test_labels.shape[0]\n",
    "                n                          = 5000\n",
    "                random_indices             = np.random.choice(samples_n, n, replace=False)\n",
    "                test_data, test_labels     = (helper._test_data[random_indices],\n",
    "                                                 helper._test_labels[random_indices])\n",
    "                test_data                  = next(helper.slice_batch(test_data, sequence_length))\n",
    "                accuracy, summary_accuracy = model.run_test_step(session, test_data, test_labels)\n",
    "                train_writer.add_summary(summary_accuracy, counter)\n",
    "                print(f'Accuracy = {accuracy:3.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
