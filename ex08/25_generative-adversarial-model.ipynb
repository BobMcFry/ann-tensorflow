{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8\n",
    "## Lets start by importing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import struct\n",
    "import os\n",
    "import sys; sys.path.insert(0, '..')\n",
    "np.random.seed(100000001)\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Reader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_GAN():\n",
    "    def __init__(self, directory):\n",
    "        self._directory = directory\n",
    "\n",
    "        self._data = self._load_binaries(\"train-images.idx3-ubyte\")\n",
    "        self._data = np.append(self._data, self._load_binaries(\"t10k-images.idx3-ubyte\"), axis = 0)\n",
    "        self._data = ((self._data / 255) * 2) - 1\n",
    "        self._data = self._data.reshape([-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "    def _load_binaries(self, file_name):\n",
    "        path = os.path.join(self._directory, file_name)\n",
    "\n",
    "        with open(path, 'rb') as fd:\n",
    "            check, items_n = struct.unpack(\">ii\", fd.read(8))\n",
    "\n",
    "            if \"images\" in file_name and check == 2051:\n",
    "                height, width = struct.unpack(\">II\", fd.read(8))\n",
    "                images = np.fromfile(fd, dtype = 'uint8')\n",
    "                return np.reshape(images, (items_n, height, width))\n",
    "            else:\n",
    "                raise ValueError(\"Not a MNIST file: \" + path)\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        samples_n = self._data.shape[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = samples_n\n",
    "\n",
    "        random_indices = np.random.choice(samples_n, samples_n, replace = False)\n",
    "        data = self._data[random_indices]\n",
    "\n",
    "        for i in range(samples_n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "            yield data[on:off]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Graph-Building Helper Functions (provided by Lukas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_layer(x, target_size, is_training, normalize = False, activation_function = None):\n",
    "    fan_in = int(x.shape[-1])\n",
    "\n",
    "    if activation_function == tf.nn.relu:\n",
    "        var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "    else:\n",
    "        var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "    weights = tf.get_variable(\"weights\", [x.shape[1], target_size], tf.float32, var_init)\n",
    "\n",
    "    var_init = tf.constant_initializer(0.0)\n",
    "    biases = tf.get_variable(\"biases\", [target_size], tf.float32, var_init)\n",
    "\n",
    "    activation = tf.matmul(x, weights) + biases\n",
    "\n",
    "    if normalize:\n",
    "        activation = batch_norm(activation, [0], is_training)\n",
    "\n",
    "    return activation_function(activation) if callable(activation_function) else activation\n",
    "\n",
    "\n",
    "def conv_layer(x, kernel_quantity, kernel_size, stride_size, normalize = False, activation_function = None, is_training=False):\n",
    "    depth = x.shape[-1]\n",
    "    fan_in = int(x.shape[1] * x.shape[2])\n",
    "\n",
    "    if activation_function == tf.nn.relu or activation_function == tf.nn.leaky_relu:\n",
    "        var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "    else:\n",
    "        var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "    kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, depth, kernel_quantity], tf.float32, var_init)\n",
    "\n",
    "    var_init = tf.constant_initializer(0.0)\n",
    "    biases = tf.get_variable(\"biases\", [kernel_quantity], initializer = var_init)\n",
    "\n",
    "    activation = tf.nn.conv2d(x, kernels, strides = [1, stride_size, stride_size, 1], padding = \"SAME\") + biases\n",
    "\n",
    "    if normalize:\n",
    "        activation = batch_norm(activation, [0, 1, 2], is_training)\n",
    "\n",
    "    return activation_function(activation) if callable(activation_function) else activation\n",
    "\n",
    "\n",
    "def transposed_conv_layer(x, kernel_size, output_shape, stride_size, is_training, normalize = False, activation_function = False):\n",
    "    deconv_filter = tf.get_variable('deconv_filter', shape=kernel_size,\n",
    "                                        initializer=tf.random_normal_initializer(0.1))\n",
    "    deconv_bias = tf.get_variable('deconv_bias', shape=output_shape[-1])\n",
    "    deconv = tf.nn.conv2d_transpose(x, deconv_filter, output_shape, strides=stride_size,\n",
    "                                        padding='SAME')\n",
    "    if normalize:\n",
    "        deconv = batch_norm(deconv + deconv_bias, [0, 1, 2], is_training)\n",
    "\n",
    "    return activation_function(deconv) if callable(activation_function) else deconv\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    size = int(np.prod(x.shape[1:]))\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n",
    "\n",
    "def _pop_batch_norm(x, pop_mean, pop_var, offset, scale):\n",
    "    return tf.nn.batch_normalization(x, pop_mean, pop_var, offset, scale, 1e-6)\n",
    "\n",
    "def _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale):\n",
    "    decay = 0.99\n",
    "\n",
    "    dependency_1 = tf.assign(pop_mean, pop_mean * decay + mean * (1 - decay))\n",
    "    dependency_2 = tf.assign(pop_var, pop_var * decay + var * (1 - decay))\n",
    "\n",
    "    with tf.control_dependencies([dependency_1, dependency_2]):\n",
    "        return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)\n",
    "\n",
    "def batch_norm(x, axes, is_training):\n",
    "    depth = x.shape[-1]\n",
    "    mean, var = tf.nn.moments(x, axes = axes)\n",
    "\n",
    "    var_init = tf.constant_initializer(0.0)\n",
    "    offset = tf.get_variable(\"offset\", [depth], tf.float32, var_init)\n",
    "    var_init = tf.constant_initializer(1.0)\n",
    "    scale = tf.get_variable(\"scale\", [depth], tf.float32, var_init)\n",
    "\n",
    "    pop_mean = tf.get_variable(\"pop_mean\", [depth], initializer = tf.zeros_initializer(), trainable = False)\n",
    "    pop_var = tf.get_variable(\"pop_var\", [depth], initializer = tf.ones_initializer(), trainable = False)\n",
    "\n",
    "    return tf.cond(\n",
    "        is_training,\n",
    "        lambda: _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale),\n",
    "        lambda: _pop_batch_norm(x, pop_mean, pop_var, offset, scale)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generative Adversarial Network Helper Class\n",
    "Here we define the class that contains all information about our GAN. We define the TF graph and also provide methods for training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    '''.. :py:class::``GAN``\n",
    "\n",
    "    This class encapsulates both generator and discriminator.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    input   :   tf.placeholder\n",
    "                Placeholder of shape (batch_size, 50) of uniform random values from [-1,1]\n",
    "    is_training :   tf.placeholder\n",
    "                    Bool placeholder for correct batch norm during inference\n",
    "    gen_output  :   tf.Tensor\n",
    "                    Output of the generator portion\n",
    "    dis_output  :   tf.Tensor\n",
    "                    Output of the discriminator portion\n",
    "    loss_dis    :   Cross-entropy loss of the discriminator\n",
    "    loss_gen    :   Cross-entropy loss of the generator\n",
    "    input_reals :   tf.placeholder\n",
    "                    Placeholder for feeding batch_size real input images\n",
    "    '''\n",
    "\n",
    "    def __init__(self, batch_size=32, learning_rate=0.0004):\n",
    "        ############################################################################################\n",
    "        #                                        GENERATOR                                         #\n",
    "        ############################################################################################\n",
    "        # note that we forego naming the ops since they are usually easily identified by their index\n",
    "        # and variable scope prefix\n",
    "        tf.reset_default_graph()\n",
    "        with tf.variable_scope('generator') as scope_gen:\n",
    "            self.input       = tf.placeholder(tf.float32, shape=(batch_size, 50))\n",
    "            self.is_training = tf.placeholder(tf.bool, shape=[])\n",
    "\n",
    "            # blow up to enough neurons\n",
    "            expanded_input   = feed_forward_layer(self.input, 64 * 4 * 4, self.is_training,\n",
    "                                                normalize=True, activation_function=tf.nn.relu)\n",
    "            with tf.variable_scope('layer1'):\n",
    "                layer_1   = tf.reshape(expanded_input, (-1, 4, 4, 64))\n",
    "\n",
    "            with tf.variable_scope('layer2'):\n",
    "                layer_2 = transposed_conv_layer(layer_1, (5, 5, 32, 64), (batch_size, 7, 7, 32),\n",
    "                                                (1, 2, 2, 1), self.is_training, normalize=True,\n",
    "                                                activation_function=tf.nn.relu)\n",
    "\n",
    "            with tf.variable_scope('layer3'):\n",
    "                layer_3 = transposed_conv_layer(layer_2, (5, 5, 16, 32), (batch_size, 14, 14, 16),\n",
    "                                                (1, 2, 2, 1), self.is_training, normalize=True,\n",
    "                                                activation_function=tf.nn.relu)\n",
    "\n",
    "            with tf.variable_scope('layer4'):\n",
    "                layer_4 = transposed_conv_layer(layer_3, (5, 5, 1, 16), (batch_size, 28, 28, 1),\n",
    "                                                (1, 2, 2, 1), self.is_training, normalize=False,\n",
    "                                                activation_function=tf.nn.tanh)\n",
    "            self.gen_output = layer_4\n",
    "\n",
    "            # collect all vars from this scope for the generator\n",
    "            variables_gen = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope_gen.name)\n",
    "\n",
    "        with tf.variable_scope('discriminator') as scope_dis:\n",
    "            self.input_reals = tf.placeholder(tf.float32, shape=(batch_size, 28, 28, 1))\n",
    "            inputs           = tf.concat([self.gen_output, self.input_reals], 0)\n",
    "            labels           = tf.concat([tf.zeros((batch_size, 1), tf.float32),\n",
    "                                          tf.ones((batch_size, 1), tf.float32)], 0)\n",
    "            act_fn = tf.nn.leaky_relu\n",
    "            with tf.variable_scope('layer1'):\n",
    "                conv1 = conv_layer(inputs, 8, 5, 2, activation_function=act_fn, normalize=False)\n",
    "            with tf.variable_scope('layer2'):\n",
    "                conv2 = conv_layer(conv1, 16, 5, 2, activation_function=act_fn, normalize=False)\n",
    "            with tf.variable_scope('layer3'):\n",
    "                conv3 = conv_layer(conv2, 32, 5, 2, activation_function=act_fn, normalize=False)\n",
    "            with tf.variable_scope('layer4'):\n",
    "                conv3_reshaped  = tf.reshape(conv3, shape=(batch_size * 2, 4 * 4 * 32))\n",
    "                self.dis_output = feed_forward_layer(conv3_reshaped,\n",
    "                                                     1,\n",
    "                                                     self.is_training,\n",
    "                                                     normalize=False,\n",
    "                                                     activation_function=tf.nn.sigmoid)\n",
    "\n",
    "            # collect all vars from this scope for the discriminator\n",
    "            variables_dis = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope_dis.name)\n",
    "\n",
    "            entropy_dis   = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=self.dis_output)\n",
    "            self.loss_dis = tf.reduce_mean(entropy_dis)\n",
    "\n",
    "            entropy_gen = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.ones((batch_size, 1), tf.float32), logits=self.dis_output[:batch_size]\n",
    "\n",
    "            )\n",
    "            self.loss_gen       = tf.reduce_mean(entropy_gen)\n",
    "            self.train_step_gen = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(self.loss_gen, var_list=variables_gen)\n",
    "            self.train_step_dis = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(self.loss_dis, var_list=variables_dis)\n",
    "\n",
    "    def train_step(self, session, vector, real_images, is_training):\n",
    "        '''Run one step of training.'''\n",
    "\n",
    "        fetches = [self.loss_dis, self.loss_gen, self.train_step_dis, self.train_step_gen]\n",
    "        feeds = {self.input: vector,\n",
    "                 self.input_reals: real_images,\n",
    "                 self.is_training: is_training}\n",
    "        loss_dis, loss_gen, _, _ = session.run(fetches, feed_dict=feeds)\n",
    "        return loss_dis, loss_gen\n",
    "\n",
    "    def generate_images(self, session, vectors):\n",
    "        fetches = self.gen_output\n",
    "        feeds = {self.input: vectors, self.is_training: False}\n",
    "        return session.run(fetches, feed_dict=feeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Printing Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(imgs):\n",
    "    n, h, w, c = imgs.shape\n",
    "    cols = int(np.sqrt(n))\n",
    "    rows = int(n / cols) + 1\n",
    "    fig, axarr = plt.subplots(rows, cols)\n",
    "    for index in range(n):\n",
    "        row = index // cols\n",
    "        col = index % cols\n",
    "        ax = axarr[row][col]\n",
    "        ax.imshow(imgs[index, :, :, 0], cmap='gray')\n",
    "\n",
    "    # delete empty plots\n",
    "    for index in range(n, rows*cols):\n",
    "        row = index // cols\n",
    "        col = index % cols\n",
    "        fig.delaxes(axarr[row][col])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_helper = MNIST_GAN('data')\n",
    "epochs       = 5\n",
    "batch_size   = 64\n",
    "gan          = GAN(batch_size)\n",
    "\n",
    "losses_dis = []\n",
    "losses_gen = []\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Starting epoch {epoch+1}/{epochs}')\n",
    "        for batch in mnist_helper.get_batch(batch_size):\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, 50))\n",
    "            loss_dis, loss_gen = gan.train_step(session, z, batch, True)\n",
    "            losses_dis.append(loss_dis)\n",
    "            losses_gen.append(loss_gen)\n",
    "    imgs = gan.generate_images(session, np.random.uniform(-1, 1, size=(batch_size, 50)))\n",
    "    plot_images(imgs)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111)\n",
    "ax.set_title('Entropy of GAN')\n",
    "ax.set_xlabel('Training step')\n",
    "ax.set_ylabel('Sigmoid cross-entropy')\n",
    "ax.plot(losses_dis, label='Discriminator')\n",
    "ax.plot(losses_gen, label='Generator')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}