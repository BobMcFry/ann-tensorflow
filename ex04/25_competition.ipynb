{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from svhn_helper import SVHN\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is necessary for the training data to reside in this notebook's directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics():\n",
    "    ############################################################################\n",
    "    #                         Print class distribution                         #\n",
    "    ############################################################################\n",
    "    print('Percentage of labels in train and validation set')\n",
    "    train_labels = svhn._training_labels\n",
    "    validation_labels = svhn._validation_labels\n",
    "    train_dist = np.histogram(train_labels)[0] / len(train_labels)\n",
    "    validation_dist = np.histogram(validation_labels)[0]  / len(validation_labels)\n",
    "    print('%15s | %10s' % ('train', 'validation'))\n",
    "    print(' ' * 5 + '-' * 23)\n",
    "    for index, (t, v) in enumerate(zip(train_dist, validation_dist)):\n",
    "        print('%5d%10f | %10f' % (index + 1, t, v))\n",
    "\n",
    "def plot_mispredictions(model, filename, data, labels):\n",
    "    import tensorflow as tf\n",
    "    with tf.Session().as_default() as session:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(session, filename)\n",
    "        validation_predictions = model.predict(session, data)\n",
    "        actual_labels = labels\n",
    "        mispredictions = np.argwhere(actual_labels != validation_predictions)\n",
    "        print(f'Number of misclassifications: {len(mispredictions):d}')\n",
    "        print(f'Percent: {len(mispredictions)/len(actual_labels)*100:f}')\n",
    "        N = 10**2\n",
    "        mislabeled_data = data[mispredictions, ...]\n",
    "        mislabeled_labels = validation_predictions[mispredictions][:, 0]\n",
    "        plot(list(zip(mislabeled_data[:N], mislabeled_labels[:N])))\n",
    "\n",
    "def plot(data):\n",
    "\n",
    "    N = len(data)\n",
    "    categories = ['0','1','2','3','4','5','6','7','8','9','0']\n",
    "    h, w = (int(np.floor(np.sqrt(N))), int(np.ceil(np.sqrt(N))))\n",
    "    f, axarr = plt.subplots(h, w)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            index = 4*i+j\n",
    "\n",
    "            ax = axarr[i][j]\n",
    "\n",
    "            img = data[index][0].reshape((32,32))\n",
    "\n",
    "            label = data[index][1]\n",
    "\n",
    "            ax.set_title(categories[label])\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    N = 100\n",
    "    N_train, N_val = svhn.get_sizes()[:2]\n",
    "    random_indices_train = np.random.choice(N_train, N, replace=False)\n",
    "    random_indices_val = np.random.choice(N_val, N, replace=False)\n",
    "\n",
    "    train_batch = list(zip(svhn._training_data[random_indices_train],\n",
    "        svhn._training_labels[random_indices_train]))\n",
    "\n",
    "    val_batch = list(zip(svhn._validation_data[random_indices_val],\n",
    "        svhn._validation_labels[random_indices_val]))\n",
    "\n",
    "    print_statistics()\n",
    "    plot(train_batch)\n",
    "    plot(val_batch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    svhn = SVHN()\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Helper\n",
    "To make this stuff easier in the future, we are creating some model helper classes and functions in order to ease the process of training and model handling.\n",
    "### Model Class\n",
    "This class acts as an abstract base class for our network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    '''Base model class.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    opt   :   str\n",
    "                    TF optimizer to use\n",
    "    act_fn  :   function\n",
    "                    tf.nn function for neuron activation\n",
    "    '''\n",
    "\n",
    "    def run_training_step(self, session, data, labels):\n",
    "        '''Run forward pass through net and apply gradients once.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        session :   tf.Session\n",
    "                    Session to use for executing everything\n",
    "        data    :   np.ndarray\n",
    "                    Input data\n",
    "        labels  :   np.ndarray\n",
    "                    Input labels\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def get_accuracy(self, session, data, labels):\n",
    "        '''Run forward pass through net and compute accuracy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        session :   tf.Session\n",
    "                    Session to use for executing everything\n",
    "        data    :   np.ndarray\n",
    "                    Input data\n",
    "        labels  :   np.ndarray\n",
    "                    Input labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def predict(self, session, data):\n",
    "        '''Get model predictions for data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        session :   tf.Session\n",
    "        data    :   np.ndarray\n",
    "                    Must fit the model's input placeholder\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of which output neuron is most active for each input\n",
    "\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def __init__(self, optimizer, activation):\n",
    "        '''Init new model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr  :   float\n",
    "                Learning rate for the optimizer\n",
    "        optimizer   :   tf.train.Optimizer\n",
    "                        TF optimizer to use\n",
    "        activation  :   function\n",
    "                        tf.nn function for neuron activation\n",
    "        '''\n",
    "        self.opt = optimizer\n",
    "        self.act_fn = activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "norm_n = 0\n",
    "\n",
    "\n",
    "def batch_norm_layer(input):\n",
    "    '''Create a layer that normalizes the batch with it's mean and variance.'''\n",
    "    global norm_n\n",
    "    norm_n += 1\n",
    "    with tf.variable_scope('norm%d' % norm_n):\n",
    "        mean, var = tf.nn.moments(input, axes=[0, 1, 1])\n",
    "        return tf.nn.batch_normalization(input, mean, var, 0, 1, 1e-10)\n",
    "\n",
    "pool_n = 0\n",
    "\n",
    "\n",
    "def max_pool_layer(input, ksize, strides):\n",
    "    global pool_n\n",
    "    pool_n += 1\n",
    "    with tf.variable_scope('pool%d' % pool_n):\n",
    "        return tf.nn.max_pool(input,\n",
    "                ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "conv_n = 0\n",
    "\n",
    "\n",
    "def conv_layer(input, kshape, strides=(1, 1, 1, 1), activation=tf.nn.tanh):\n",
    "    '''Create a convolutional layer with fixed activation function and variable\n",
    "    initialisation. The activation function is ``tf.nn.tanh`` and variables are\n",
    "    initialised from a truncated normal distribution with an stddev of 0.1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input   :   tf.Variable\n",
    "                Input to the layer\n",
    "    kshape  :   tuple or list\n",
    "                Shape of the kernel tensor\n",
    "    strides :   tuple or list\n",
    "                Strides\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "            The variable representing the layer activation (tanh(conv + bias))\n",
    "\n",
    "    '''\n",
    "    global conv_n\n",
    "    conv_n += 1\n",
    "    # this adds a prefix to all variable names\n",
    "    with tf.variable_scope('conv%d' % conv_n):\n",
    "        kernels = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                kshape,\n",
    "                stddev=0.1),\n",
    "            kshape, name='kernels')\n",
    "        bias_shape = (kshape[-1],)\n",
    "        biases = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                bias_shape,\n",
    "                stddev=0.1), name='bias')\n",
    "        conv = tf.nn.conv2d(\n",
    "            input,\n",
    "            kernels,\n",
    "            strides,\n",
    "            padding='SAME',\n",
    "            name='conv')\n",
    "        return activation(tf.nn.tanh(conv + biases, name='activation'))\n",
    "\n",
    "\n",
    "fc_n = 0\n",
    "\n",
    "\n",
    "def fully_connected(input, n_out, with_activation=False, activation=tf.nn.tanh):\n",
    "    '''Create a fully connected layer with fixed activation function and variable\n",
    "    initialisation. The activation function is ``tf.nn.tanh`` and variables are\n",
    "    initialised from a truncated normal distribution with an stddev of 0.1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input   :   tf.Variable\n",
    "                Input to the layer\n",
    "    n_out   :   int\n",
    "                Number of neurons in the layer\n",
    "    with_activation :   bool\n",
    "                        Return activation or drive (useful when planning to use\n",
    "                        ``softmax_cross_entropy_with_logits`` which requires\n",
    "                        unscaled logits)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "            The variable representing the layer activation (tanh(input * Weights\n",
    "            + bias))\n",
    "    '''\n",
    "    global fc_n\n",
    "    fc_n += 1\n",
    "    with tf.variable_scope('fully%d' % fc_n):\n",
    "        init = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        W = tf.get_variable(\n",
    "                'weights',\n",
    "                initializer=init,\n",
    "                shape=(input.shape[-1], n_out), # the last dim of the input\n",
    "               dtype=tf.float32                 # is the 1st dim of the weights\n",
    "            )\n",
    "        bias = tf.get_variable('bias', initializer=init, shape=(n_out,))\n",
    "        drive = tf.matmul(input, W) + bias\n",
    "        if with_activation:\n",
    "            return activation(drive)\n",
    "        else:\n",
    "            return drive\n",
    "\n",
    "\n",
    "\n",
    "weighted_pool_n = 0\n",
    "\n",
    "\n",
    "def weighted_pool_layer(input_layer, ksize, strides=(1, 1, 1, 1)):\n",
    "    '''Helper function to do mixed max/avg pooling\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_layer :   tf.Tensor\n",
    "                    4D tensor\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "           Tthe 4D tensor after being pooled\n",
    "    '''\n",
    "    global weighted_pool_n\n",
    "    weighted_pool_n += 1\n",
    "    with tf.variable_scope('weight_pool%d' % weighted_pool_n):\n",
    "        a = tf.get_variable('a',\n",
    "                initializer=tf.truncated_normal_initializer(),\n",
    "                shape=(1,),\n",
    "                dtype=tf.float32, trainable=True)\n",
    "        max_pool = tf.nn.max_pool(input_layer, ksize, strides, padding='SAME')\n",
    "        avg_pool = tf.nn.avg_pool(input_layer, ksize, strides, padding='SAME')\n",
    "        pool = (a * max_pool + (1 - a) * avg_pool)\n",
    "        return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can be used to train the model on a set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterTest(object):\n",
    "    '''Test one set of parameters to the train() function.'''\n",
    "    def __init__(self, model, batch_size, epochs,\n",
    "            train_function, learning_rate, ignore_saved):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.accuracy = None\n",
    "        self.train_function=train_function\n",
    "        # sadly, we cannot always retrieve this from any optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ignore_saved = ignore_saved\n",
    "\n",
    "    def run(self):\n",
    "        '''Run the training process with the specified settings.'''\n",
    "\n",
    "        self.save_fname = 'checkpoints/{name}_{batch}_{lr}_{epochs}_{opti}_{act}.ckpt'.format(\n",
    "                name=self.model.__class__.__name__,\n",
    "                batch=self.batch_size,\n",
    "                lr=self.learning_rate,\n",
    "                epochs=self.epochs,\n",
    "                opti=self.model.opt.get_name(),\n",
    "                act=self.model.act_fn.__name__\n",
    "        )\n",
    "        self.accuracy = self.train_function(self.model, self.batch_size,\n",
    "                self.epochs, self.save_fname, return_records=False,\n",
    "                record_step=30, ignore_saved=self.ignore_saved)\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('{opti:30}, learning rate={lr:5.4f}, batch size={bs:<5d}, '\n",
    "                'epochs={epochs:<5d}, accuracy={acc:4.3f}'.format(\n",
    "                    lr=self.learning_rate,\n",
    "                    opti=self.model.opt.get_name(),\n",
    "                    bs=self.batch_size,\n",
    "                    epochs=self.epochs,\n",
    "                    acc=self.accuracy\n",
    "                    )\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training model\n",
    "Here we are creating our actual training model which basically implements the functions defined in our abstract model class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseModel):\n",
    "    '''Smaller model so we clock in at < 4mb'''\n",
    "\n",
    "    def predict(self, session, data):\n",
    "        return session.run([self.prediction], feed_dict={self.x: data,\n",
    "            self.y_: np.zeros(data.shape[0])})[0]\n",
    "\n",
    "    def run_training_step(self, session, data, labels):\n",
    "        entropy, _ = session.run(\n",
    "            [self.mean_cross_entropy, self.train_step],\n",
    "            feed_dict={self.x: data, self.y_: labels})\n",
    "        return entropy\n",
    "\n",
    "    def get_accuracy(self, session, data, labels):\n",
    "        return session.run([self.accuracy], feed_dict={self.x: data, self.y_:\n",
    "            labels})[0]\n",
    "\n",
    "\n",
    "       def __init__(self, optimizer, activation):\n",
    "        super().__init__(optimizer, activation)\n",
    "\n",
    "        ############################################################################\n",
    "        #                             Define the graph                             #\n",
    "        ############################################################################\n",
    "        # It turns out that this network from ex03 is already capable of memorizing\n",
    "        # the entire training or validation set, so we need to tweak generalization,\n",
    "        # not capacity\n",
    "        # In order to speed up convergence, we added batch normalization.\n",
    "        # Our best effort was Adam optimizer with bs=32, lr=0.001 (only possible\n",
    "        # because of norm)\n",
    "        x = tf.placeholder(tf.float32, shape=(None, 32, 32, 1), name='input')\n",
    "        y_ = tf.placeholder(dtype=tf.int32, shape=(None,), name='labels')\n",
    "\n",
    "        self.x = x\n",
    "        self.y_ = y_\n",
    "\n",
    "        kernel_shape1 = (5, 5, 1, 8)\n",
    "        activation1 = conv_layer(x, kernel_shape1, activation=activation)\n",
    "\n",
    "        normalize1 = batch_norm_layer(activation1)\n",
    "\n",
    "        pool1 = weighted_pool_layer(\n",
    "            normalize1, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1)\n",
    "        )\n",
    "\n",
    "        kernel_shape2 = (3, 3, 8, 10)\n",
    "        activation2 = conv_layer(pool1, kernel_shape2, activation=activation)\n",
    "\n",
    "        normalize2 = batch_norm_layer(activation2)\n",
    "\n",
    "        pool2 = weighted_pool_layer(\n",
    "            normalize2, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1)\n",
    "        )\n",
    "\n",
    "        pool2_reshaped = tf.reshape(pool2, (-1, 8*8*10), name='reshaped1')\n",
    "        fc1 = fully_connected(pool2_reshaped, 512, with_activation=True,\n",
    "                activation=activation)\n",
    "\n",
    "        fc2_logit = fully_connected(fc1, 10, activation=activation)\n",
    "\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=fc2_logit,\n",
    "                                                                labels=y_)\n",
    "        mean_cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "        self.mean_cross_entropy = mean_cross_entropy\n",
    "        train_step = optimizer.minimize(mean_cross_entropy)\n",
    "        self.train_step = train_step\n",
    "        self.prediction = tf.argmax(fc2_logit, 1, output_type=tf.int32)\n",
    "\n",
    "        # check if neuron firing strongest coincides with max value position in real\n",
    "        # labels\n",
    "        correct_prediction = tf.equal(self.prediction, y_)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        self.accuracy = accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functionality\n",
    "Here we are creating a function that will train our network. Additionally the trained weights are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, epochs, save_fname, return_records=False,\n",
    "        record_step=20, ignore_saved=False):\n",
    "    '''Train a model on the SVHN dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model           :   Model (defined above)\n",
    "                        The training model.\n",
    "    batch_size      :   int\n",
    "                        Size of training batch.\n",
    "    epochs          :   int\n",
    "                        Number of times to visit the entire training set.\n",
    "    save_fname      :   string\n",
    "                        The filename of the file carrying all the learned\n",
    "                        variables.\n",
    "    return_records  :   bool\n",
    "                        Determines whether only the final accuracy (False) or a\n",
    "                        history of all entropies and accuracies is returned.\n",
    "    record_step     :   int\n",
    "                        Accuracy on test set will be recorded every\n",
    "                        ``record_step`` training steps.\n",
    "    ignore_saved    :   bool\n",
    "                        Do not load saved weights, if found\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float OR tuple\n",
    "            If ``return_records`` is set, all entropies and accuracies are\n",
    "            returned. Else the best accuracy is returned.\n",
    "\n",
    "    '''\n",
    "\n",
    "    svhn = SVHN()\n",
    "\n",
    "    # keeep records of performance\n",
    "    entropies = []\n",
    "    accuracies = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        ########################################################################\n",
    "        #                             Load weights                             #\n",
    "        ########################################################################\n",
    "        saver = tf.train.Saver()\n",
    "        if not ignore_saved and os.path.exists(save_fname + '.meta'):\n",
    "            print('Using saved weights.')\n",
    "            saver.restore(sess, save_fname)\n",
    "            final_accuracy = model.get_accuracy(sess, svhn._validation_data,\n",
    "                                svhn._validation_labels)\n",
    "            return final_accuracy\n",
    "        else:\n",
    "            ####################################################################\n",
    "            #                             Training                             #\n",
    "            ####################################################################\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # number of training steps\n",
    "            training_step = 0\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                print('Starting epoch %d' % epoch)\n",
    "\n",
    "                # run one batch\n",
    "                for data, labels in svhn.get_training_batch(batch_size):\n",
    "                    entropy = model.run_training_step(sess, data, labels)\n",
    "                    entropies.append(entropy)\n",
    "\n",
    "                    # compute validation accuracy every record_step steps\n",
    "                    if training_step % record_step == 0:\n",
    "                        val_accuracy = model.get_accuracy(sess, svhn._validation_data,\n",
    "                                svhn._validation_labels)\n",
    "                        accuracies.append(val_accuracy)\n",
    "                        # in case we need it later\n",
    "                        final_accuracy = val_accuracy\n",
    "                        print('Current validation accuracy %f' % val_accuracy)\n",
    "\n",
    "                        # save if better\n",
    "                        if val_accuracy > best_accuracy:\n",
    "                            best_accuracy = val_accuracy\n",
    "                            print('Saving model with accuracy %f.' % val_accuracy)\n",
    "                            saver.save(sess, save_fname)\n",
    "\n",
    "                    training_step += 1\n",
    "\n",
    "                    # stop early if convergence too slow\n",
    "                    if epoch == 1:\n",
    "                        if val_accuracy < 0.2:\n",
    "                            raise RuntimeError('This isn\\'t going anywhere.')\n",
    "\n",
    "            ####################################################################\n",
    "            #               Make final recordings, if necessary                #\n",
    "            ####################################################################\n",
    "            if training_step % record_step == 1:\n",
    "                # we just recorded, final_accuracy already correct\n",
    "                pass\n",
    "            else:\n",
    "                # we need to recompute the final accuracy\n",
    "                final_accuracy = model.get_accuracy(sess, svhn._validation_data,\n",
    "                                    svhn._validation_labels)\n",
    "                accuracies.append(final_accuracy)\n",
    "\n",
    "            ####################################################################\n",
    "            #                     Print misclassifications                     #\n",
    "            ####################################################################\n",
    "            if return_records:\n",
    "                return entropies, accuracies\n",
    "            else:\n",
    "                return best_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer_cls  = getattr(tf.train, 'AdamOptimizer')\n",
    "optimizer      = optimizer_cls(0.001)\n",
    "model          = Model(optimizer, tf.nn.relu)\n",
    "epochs         = 15\n",
    "batch_size     = 32\n",
    "entropies, accuracies = train_model(model, batch_size, epochs, \"./weights/\", True, 20, ignore_saved=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot entropies and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(entropies)\n",
    "plt.title('Entropy')\n",
    "plt.xlabel('Train step')\n",
    "plt.ylabel('Entropy')\n",
    "plt.show()\n",
    "plt.title('Accuracy')\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel('Record step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Somehow, we don't get how tensorflow treats the files generated by `saver.save()`. Within this notebook, it refuses to load. You'll have to figure out by yourself, how to properly load them, or take a look at out `*.py` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session,  tf.train.latest_checkpoint(\"./weights/\"))\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    for step, (images, labels) in enumerate(svhn.get_test_batch(300)):\n",
    "        test_accuracy += session.run(\n",
    "            accuracy,\n",
    "            feed_dict = {x: images, desired: labels}\n",
    "        )\n",
    "    \n",
    "print(\"Test Accuracy: \" + str(test_accuracy / step))"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}