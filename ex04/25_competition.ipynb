{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from svhn_helper import SVHN\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Data loader object\n",
    "svhn = SVHN()\n",
    "\n",
    "def print_statistics():\n",
    "    ############################################################################\n",
    "    #                         Print class distribution                         #\n",
    "    ############################################################################\n",
    "    train_labels      = svhn._training_labels\n",
    "    validation_labels = svhn._validation_labels\n",
    "    train_dist        = np.histogram(train_labels)[0] / len(train_labels)\n",
    "    validation_dist   = np.histogram(validation_labels)[0]  / len(validation_labels)\n",
    "    print('Percentage of labels in train and validation set')\n",
    "    print('%15s | %10s' % ('train', 'validation'))\n",
    "    print(' ' * 5 + '-' * 23)\n",
    "    for index, (t, v) in enumerate(zip(train_dist, validation_dist)):\n",
    "        print('%5d%10f | %10f' % (index + 1, t, v))\n",
    "\n",
    "def plot(N=100):\n",
    "\n",
    "    N_train, N_val = svhn.get_sizes()[:2]\n",
    "    random_indices_train = np.random.choice(N_train, N, replace=False)\n",
    "    random_indices_val = np.random.choice(N_val, N, replace=False)\n",
    "\n",
    "    train_batch = list(zip(svhn._training_data[random_indices_train],\n",
    "        svhn._training_labels[random_indices_train]))\n",
    "\n",
    "    val_batch = list(zip(svhn._validation_data[random_indices_val],\n",
    "        svhn._validation_labels[random_indices_val]))\n",
    "\n",
    "    categories = ['0','1','2','3','4','5','6','7','8','9','0']\n",
    "    h, w = (int(np.floor(np.sqrt(N))), int(np.ceil(np.sqrt(N))))\n",
    "    f_train, axarr_train = plt.subplots(h, w)\n",
    "    f_val, axarr_val = plt.subplots(h, w)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            index = 4*i+j\n",
    "\n",
    "            ax_train = axarr_train[i][j]\n",
    "            ax_val = axarr_val[i][j]\n",
    "\n",
    "            img_train = train_batch[index][0].reshape((32,32))\n",
    "            img_val = val_batch[index][0].reshape((32,32))\n",
    "\n",
    "            label_train = train_batch[index][1]\n",
    "            label_val = val_batch[index][1]\n",
    "\n",
    "            ax_train.set_title(categories[label_train])\n",
    "            ax_train.imshow(img_train, cmap='gray')\n",
    "            ax_train.get_xaxis().set_visible(False)\n",
    "            ax_train.get_yaxis().set_visible(False)\n",
    "\n",
    "            ax_val.set_title(categories[label_val])\n",
    "            ax_val.imshow(img_val, cmap='gray')\n",
    "            ax_val.get_xaxis().set_visible(False)\n",
    "            ax_val.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "print_statistics()\n",
    "plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Helper\n",
    "Now we are creating some model helper classes and functions in order to ease the process of training and model handling.\n",
    "### Model Class\n",
    "This class acts as an abstract base class for our network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    '''Base model class.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    opt   :   str\n",
    "                    TF optimizer to use\n",
    "    act_fn  :   function\n",
    "                    tf.nn function for neuron activation\n",
    "    '''\n",
    "\n",
    "    def run_training_step(self, session, data, labels):\n",
    "        '''Run forward pass through net and apply gradients once.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        session :   tf.Session\n",
    "                    Session to use for executing everything\n",
    "        data    :   np.ndarray\n",
    "                    Input data\n",
    "        labels  :   np.ndarray\n",
    "                    Input labels\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def get_accuracy(self, session, data, labels):\n",
    "        '''Run forward pass through net and compute accuracy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        session :   tf.Session\n",
    "                    Session to use for executing everything\n",
    "        data    :   np.ndarray\n",
    "                    Input data\n",
    "        labels  :   np.ndarray\n",
    "                    Input labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def __init__(self, optimizer, activation):\n",
    "        '''Init new model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr  :   float\n",
    "                Learning rate for the optimizer\n",
    "        optimizer   :   tf.train.Optimizer\n",
    "                        TF optimizer to use\n",
    "        activation  :   function\n",
    "                        tf.nn function for neuron activation\n",
    "        '''\n",
    "        self.opt = optimizer\n",
    "        self.act_fn = activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "SEED = 5\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "pool_n = 0\n",
    "\n",
    "\n",
    "def max_pool_layer(input, ksize, strides):\n",
    "    global pool_n\n",
    "    pool_n += 1\n",
    "    with tf.variable_scope('pool%d' % pool_n):\n",
    "        return tf.nn.max_pool(input,\n",
    "                ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "conv_n = 0\n",
    "\n",
    "\n",
    "def conv_layer(input, kshape, strides=(1, 1, 1, 1), activation=tf.nn.tanh):\n",
    "    '''Create a convolutional layer with fixed activation function and variable\n",
    "    initialisation. The activation function is ``tf.nn.tanh`` and variables are\n",
    "    initialised from a truncated normal distribution with an stddev of 0.1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input   :   tf.Variable\n",
    "                Input to the layer\n",
    "    kshape  :   tuple or list\n",
    "                Shape of the kernel tensor\n",
    "    strides :   tuple or list\n",
    "                Strides\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "            The variable representing the layer activation (tanh(conv + bias))\n",
    "\n",
    "    '''\n",
    "    global conv_n\n",
    "    conv_n += 1\n",
    "    # this adds a prefix to all variable names\n",
    "    with tf.variable_scope('conv%d' % conv_n):\n",
    "        kernels = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                kshape,\n",
    "                stddev=0.1),\n",
    "            kshape, name='kernels')\n",
    "        bias_shape = (kshape[-1],)\n",
    "        biases = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                bias_shape,\n",
    "                stddev=0.1), name='bias')\n",
    "        conv = tf.nn.conv2d(\n",
    "            input,\n",
    "            kernels,\n",
    "            strides,\n",
    "            padding='SAME',\n",
    "            name='conv')\n",
    "        return activation(tf.nn.tanh(conv + biases, name='activation'))\n",
    "\n",
    "\n",
    "# counter for autmatically creating fully-connected layer variable names\n",
    "fc_n = 0\n",
    "\n",
    "\n",
    "def fully_connected(input, n_out, with_activation=False, activation=tf.nn.tanh):\n",
    "    '''Create a fully connected layer with fixed activation function and variable\n",
    "    initialisation. The activation function is ``tf.nn.tanh`` and variables are\n",
    "    initialised from a truncated normal distribution with an stddev of 0.1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input   :   tf.Variable\n",
    "                Input to the layer\n",
    "    n_out   :   int\n",
    "                Number of neurons in the layer\n",
    "    with_activation :   bool\n",
    "                        Return activation or drive (useful when planning to use\n",
    "                        ``softmax_cross_entropy_with_logits`` which requires\n",
    "                        unscaled logits)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "            The variable representing the layer activation (tanh(input * Weights\n",
    "            + bias))\n",
    "    '''\n",
    "    global fc_n\n",
    "    fc_n += 1\n",
    "    with tf.variable_scope('fully%d' % fc_n):\n",
    "        init = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        W = tf.get_variable(\n",
    "                'weights',\n",
    "                initializer=init,\n",
    "                shape=(input.shape[-1], n_out), # the last dim of the input\n",
    "               dtype=tf.float32                 # is the 1st dim of the weights\n",
    "            )\n",
    "        bias = tf.get_variable('bias', initializer=init, shape=(n_out,))\n",
    "        drive = tf.matmul(input, W) + bias\n",
    "        if with_activation:\n",
    "            return activation(drive)\n",
    "        else:\n",
    "            return drive\n",
    "\n",
    "# counter for autmatically creating fully-connected layer variable names\n",
    "bn_n = 0\n",
    "\n",
    "\n",
    "def batch_normalization_layer(input_layer, dimension):\n",
    "    '''Helper function to do batch normalziation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_layer :   tf.Tensor\n",
    "                    4D tensor\n",
    "    dimension   :   int\n",
    "                    input_layer.get_shape().as_list()[-1]. The depth of the 4D tensor\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "           Tthe 4D tensor after being normalized\n",
    "    '''\n",
    "    global bn_n\n",
    "    bn_n += 1\n",
    "    with tf.variable_scope('batch_norm%d' % bn_n):\n",
    "        mean, variance = tf.nn.moments(input_layer, axes=[0, 1, 2])\n",
    "        # normalise by mean and variance, and use no offset or scaling (0, 1)\n",
    "        bn_layer = tf.nn.batch_normalization(input_layer, mean, variance, None,\n",
    "                None, 0.001)\n",
    "\n",
    "        return bn_layer\n",
    "\n",
    "# counter for autmatically creating fully-connected layer variable names\n",
    "weighted_pool_n = 0\n",
    "\n",
    "\n",
    "def weighted_pool_layer(input_layer, ksize, strides=(1, 1, 1, 1)):\n",
    "    '''Helper function to do mixed max/avg pooling\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_layer :   tf.Tensor\n",
    "                    4D tensor\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "           Tthe 4D tensor after being pooled\n",
    "    '''\n",
    "    global weighted_pool_n\n",
    "    weighted_pool_n += 1\n",
    "    with tf.variable_scope('weight_pool%d' % weighted_pool_n):\n",
    "        a = tf.get_variable('a',\n",
    "                initializer=tf.truncated_normal_initializer(),\n",
    "                shape=(1,),\n",
    "                dtype=tf.float32, trainable=True)\n",
    "        max_pool = tf.nn.max_pool(input_layer, ksize, strides, padding='SAME')\n",
    "        avg_pool = tf.nn.avg_pool(input_layer, ksize, strides, padding='SAME')\n",
    "        pool = (a * max_pool + (1 - a) * avg_pool)\n",
    "        return pool\n",
    "\n",
    "inc_n = 0\n",
    "\n",
    "def inception2d(x, in_channels, filter_count):\n",
    "    '''Helper function to create inception module\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels :   int\n",
    "                    number of input channels\n",
    "    filter_count    :   int\n",
    "                        number of filters to use for soemthing ?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "           Tensor with filter_count*3 +1 output channels\n",
    "    '''\n",
    "    global inc_n\n",
    "    inc_n += 1\n",
    "    with tf.variable_scope('inception%d' % inc_n):\n",
    "        # bias dimension = 3*filter_count and then the extra in_channels for the avg\n",
    "        # pooling\n",
    "        bias = tf.Variable(tf.truncated_normal([3*filter_count + in_channels]))\n",
    "\n",
    "        # 1x1\n",
    "        one_filter = tf.Variable(tf.truncated_normal([1, 1, in_channels,\n",
    "            filter_count]))\n",
    "        one_by_one = tf.nn.conv2d(x, one_filter,\n",
    "                    strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        # 3x3\n",
    "        three_filter = tf.Variable(tf.truncated_normal([3, 3, in_channels,\n",
    "            filter_count]))\n",
    "        three_by_three = tf.nn.conv2d(x,\n",
    "                    three_filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        # 5x5\n",
    "        five_filter = tf.Variable(tf.truncated_normal([5, 5, in_channels,\n",
    "            filter_count]))\n",
    "        five_by_five = tf.nn.conv2d(x, five_filter,\n",
    "                    strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        # avg pooling\n",
    "        pooling = tf.nn.avg_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1],\n",
    "                padding='SAME')\n",
    "\n",
    "        x = tf.concat([one_by_one, three_by_three, five_by_five, pooling], axis=3)\n",
    "        # Concat in the 4th dim to stack\n",
    "        x = tf.nn.bias_add(x, bias)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "class ParameterTest(object):\n",
    "    '''Test one set of parameters to the train() function.'''\n",
    "    def __init__(self, model, batch_size, epochs,\n",
    "            train_function):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.accuracy = None\n",
    "        self.train_function=train_function\n",
    "\n",
    "    def run(self):\n",
    "        '''Run the training process with the specified settings.'''\n",
    "\n",
    "        save_fname = '{name}_{batch}_{lr}_{epochs}_{opti}_{act}.ckpt'.format(\n",
    "                name   = self.model.__class__.__name__,\n",
    "                batch  = self.batch_size,\n",
    "                lr     = self.model.opt._learning_rate,\n",
    "                epochs = self.epochs,\n",
    "                opti   = self.model.opt.get_name(),\n",
    "                act    = self.model.act_fn.__name__\n",
    "        )\n",
    "        self.accuracy = self.train_function(self.model, self.batch_size,\n",
    "                self.epochs, save_fname, return_records=False, record_step=30)\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('{opti:30}, learning rate={lr:5.4f}, batch size={bs:<5d}, '\n",
    "                'epochs={epochs:<5d}, accuracy={acc:4.3f}'.format(\n",
    "                    opti   = self.model.opt,\n",
    "                    lr     = self.model.lr,\n",
    "                    bs     = self.batch_size,\n",
    "                    epochs = self.epochs,\n",
    "                    acc    = self.accuracy\n",
    "                )\n",
    "        )\n",
    "\n",
    "# def main():\n",
    "#     tf_optimizers = {class_name[:-len('Optimizer')] for class_name in dir(tf.train) if 'Optimizer'\n",
    "#             in class_name and class_name != 'Optimizer'}\n",
    "#     parser = argparse.ArgumentParser(description='Test the net on one parameter set')\n",
    "#     parser.add_argument('-o', '--optimizer', required=True, type=str,\n",
    "#             choices=tf_optimizers, help='Optimization algorithm')\n",
    "#     parser.add_argument('-l', '--learning-rate', required=True, type=float,\n",
    "#             help='Learning rate for the optimizer')\n",
    "#     parser.add_argument('-b', '--batch-size', required=True, type=int,\n",
    "#             help='Batch size')\n",
    "#     parser.add_argument('-e', '--epochs', required=True, type=int,\n",
    "#             help='Number of epochs')\n",
    "#     parser.add_argument('-f', '--file', required=True, type=str,\n",
    "#             help='File to write result to')\n",
    "#     parser.add_argument('-m', '--model', required=True, type=str,\n",
    "#             help='Package path where Model class is located')\n",
    "#     parser.add_argument('-t', '--train', required=True, type=str,\n",
    "#             help='Module to search for train_model() function.')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     model_cls = __import__(args.model, globals(), locals(), ['Model']).Model\n",
    "#     train_fn = __import__(args.train, globals(), locals(),\n",
    "#             ['train_model']).train_model\n",
    "\n",
    "#     optimizer_cls = getattr(tf.train, args.optimizer + 'Optimizer')\n",
    "#     optimizer = optimizer_cls(args.learning_rate)\n",
    "#     model = model_cls(optimizer, tf.nn.relu)\n",
    "\n",
    "#     pt = ParameterTest(model, args.batch_size, args.epochs, train_fn)\n",
    "#     pt.run()\n",
    "#     print(pt)\n",
    "#     # the OS ensures sequential writes with concurrent processes\n",
    "#     with open(args.file, 'a') as f:\n",
    "#         f.write(str(pt) + '\\n')\n",
    "#         f.flush()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training model\n",
    "Here we are creating our actual training model which basically implements the functions defined in our abstract model class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseModel):\n",
    "    '''Smaller model so we clock in at < 4mb'''\n",
    "\n",
    "    def run_training_step(self, session, data, labels):\n",
    "        '''Docs --> see BaseModel'''\n",
    "        entropy, _ = session.run(\n",
    "            [self.mean_cross_entropy, self.train_step],\n",
    "            feed_dict={self.x: data, self.y_: labels})\n",
    "        return entropy\n",
    "\n",
    "    def get_accuracy(self, session, data, labels):\n",
    "        '''Docs --> see BaseModel'''\n",
    "        return session.run([self.accuracy], feed_dict={self.x: data, self.y_:\n",
    "            labels})[0]\n",
    "\n",
    "\n",
    "    def __init__(self, optimizer, activation):\n",
    "        '''Docs --> see BaseModel'''\n",
    "        super().__init__(optimizer, activation)\n",
    "\n",
    "        ############################################################################\n",
    "        #                             Define the graph                             #\n",
    "        ############################################################################\n",
    "        # It turns out that this network from ex03 is already capable of memorizing\n",
    "        # the entire training or validation set, so we need to tweak generalization,\n",
    "        # not capacity\n",
    "        x  = tf.placeholder(tf.float32, shape=(None, 32, 32, 1), name='input')\n",
    "        y_ = tf.placeholder(dtype=tf.int32, shape=(None,), name='labels')\n",
    "\n",
    "        self.x = x\n",
    "        self.y_ = y_\n",
    "\n",
    "        kernel_shape1 = (8, 8, 1, 8)\n",
    "        activation1 = conv_layer(x, kernel_shape1, activation=activation)\n",
    "\n",
    "        pool1 = weighted_pool_layer(\n",
    "            activation1, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1)\n",
    "        )\n",
    "\n",
    "        kernel_shape2 = (3, 3, 8, 10)\n",
    "        activation2 = conv_layer(pool1, kernel_shape2, activation=activation)\n",
    "\n",
    "        pool2 = weighted_pool_layer(\n",
    "            activation2, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1)\n",
    "        )\n",
    "\n",
    "        pool2_reshaped = tf.reshape(pool2, (-1, 8*8*10), name='reshaped1')\n",
    "        fc1 = fully_connected(pool2_reshaped, 512, with_activation=True,\n",
    "                activation=activation)\n",
    "\n",
    "        fc2_logit = fully_connected(fc1, 10, activation=activation)\n",
    "\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=fc2_logit,\n",
    "                                                                labels=y_)\n",
    "        mean_cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "        self.mean_cross_entropy = mean_cross_entropy\n",
    "        train_step = optimizer.minimize(mean_cross_entropy)\n",
    "        self.train_step = train_step\n",
    "\n",
    "        # check if neuron firing strongest coincides with max value position in real\n",
    "        # labels\n",
    "        correct_prediction = tf.equal(tf.argmax(fc2_logit, 1, output_type=tf.int32), y_)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        self.accuracy = accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functionality\n",
    "Here we are creating a function that will train our network. Additionally the trained weights are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, epochs, save_fname, return_records=False, record_step=20):\n",
    "    '''Train a model on the SVHN dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model           :   Model (defined above)\n",
    "                        The training model.\n",
    "    batch_size      :   int\n",
    "                        Size of training batch.\n",
    "    epochs          :   int\n",
    "                        Number of times to visit the entire training set.\n",
    "    save_fname      :   string\n",
    "                        The filename of the file carrying all the learned variables.\n",
    "    return_records  :   boolean\n",
    "                        Determines whether only the final accuracy (False) or a history of all \n",
    "                        entropies and accuracies is returned.\n",
    "    record_step     :   int\n",
    "                        Accuracy on test set will be recorded every ``record_step`` training steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scalar OR tuple\n",
    "            If ``return_records`` is set, all entropies and accuracies are returned. Else the final\n",
    "            accuracy is returned.\n",
    "    '''\n",
    "    \n",
    "    # Get the data loader\n",
    "    svhn = SVHN()\n",
    "\n",
    "    # Keep records of performance\n",
    "    entropies = []\n",
    "    accuracies = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        if os.path.exists(save_fname + '.meta'):\n",
    "            print('Using saved weights.')\n",
    "            saver.restore(sess, save_fname)\n",
    "            final_accuracy = model.get_accuracy(sess, svhn._validation_data, svhn._validation_labels)\n",
    "            return final_accuracy\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # number of training steps\n",
    "            training_step = 0\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                print('Starting epoch %d' % epoch)\n",
    "\n",
    "                # run one batch\n",
    "                for data, labels in svhn.get_training_batch(batch_size):\n",
    "                    entropy = model.run_training_step(sess, data, labels)\n",
    "                    entropies.append(entropy)\n",
    "\n",
    "                    # compute validation accuracy every record_step steps\n",
    "                    if training_step % record_step == 0:\n",
    "                        val_accuracy = model.get_accuracy(sess, svhn._validation_data, svhn._validation_labels)\n",
    "                        accuracies.append(val_accuracy)\n",
    "                        # in case we need it later\n",
    "                        final_accuracy = val_accuracy\n",
    "                        print('Current validation accuracy %f' % val_accuracy)\n",
    "\n",
    "                        # save if better\n",
    "                        if val_accuracy > best_accuracy:\n",
    "                            print('Saving model.')\n",
    "                            best_accuracy = val_accuracy\n",
    "                            saver.save(sess, save_fname)\n",
    "\n",
    "                    training_step += 1\n",
    "\n",
    "                    # stop early if convergence too slow\n",
    "                    # if epoch == 1:\n",
    "                    #     if val_accuracy < 0.2:\n",
    "                    #         raise RuntimeError('This isn\\'t going anywhere.')\n",
    "\n",
    "            if training_step % record_step == 1:\n",
    "                # we just recorded, final_accuracy already correct\n",
    "                pass\n",
    "            else:\n",
    "                # we need to recompute the final accuracy\n",
    "                final_accuracy = model.get_accuracy(sess, svhn._validation_data,\n",
    "                                    svhn._validation_labels)\n",
    "                accuracies.append(final_accuracy)\n",
    "\n",
    "            if return_records:\n",
    "                return entropies, accuracies\n",
    "            else:\n",
    "                return final_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session,  tf.train.latest_checkpoint(\"./weights/\"))\n",
    "    \n",
    "    test_accuracy = 0\n",
    "    for step, (images, labels) in enumerate(svhn.get_test_batch(300)):\n",
    "        test_accuracy += session.run(\n",
    "            accuracy,\n",
    "            feed_dict = {x: images, desired: labels}\n",
    "        )\n",
    "    \n",
    "print(\"Test Accuracy: \" + str(test_accuracy / step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}