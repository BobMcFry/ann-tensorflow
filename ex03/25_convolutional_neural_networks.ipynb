{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 3\n",
    "## Exercise 1 + 2\n",
    "### Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar_helper import CIFAR # note we renamed the helper file so it doesn't syntax error\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a CIFAR data loader\n",
    "We are using the one that was provided. We had to import numpy there because it was missing as a dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = CIFAR(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3 - Investigating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data to inspect.\n",
    "test_batch = list(zip(cifar._training_data[:16], cifar._training_labels[:16]))\n",
    "\n",
    "# Define the categories that correspond to the numeric labels.\n",
    "categories = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse',\n",
    "        'Ship', 'Truck']\n",
    "f, axarr = plt.subplots(4, 4)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        index = 4 * i + j\n",
    "        ax = axarr[i][j]\n",
    "        img = test_batch[index][0]\n",
    "        label = test_batch[index][1]\n",
    "        ax.set_title(categories[label])\n",
    "        ax.imshow(img)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!!TODO!!! Exercise 4\n",
    "TODO: How many neurons are simulated? How many degrees of freedom (weights) does the network have? Of many floating- point operations are necessary for a forward pass of the network? (Write down your answer!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Now we are implementing the DFG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph in order to cope with multiple cell executions.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define placeholders for both the input x and the labels l.\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input')\n",
    "l = tf.placeholder(dtype=tf.uint8, shape=(None, 1), name='labels')\n",
    "# The labels are encoded as one-hot vectors.\n",
    "l_one_hot = tf.squeeze(tf.one_hot(l, 10), axis=1)\n",
    "\n",
    "\n",
    "#############\n",
    "# LAYER ONE #\n",
    "#############\n",
    "\n",
    "# There are 16 Kernels with a shape of 5x5x3 each, plus a bias.\n",
    "kernel_shape1 = (5, 5, 3, 16)\n",
    "bias_shape1   = (16,)\n",
    "\n",
    "# Now we initialize the kernel with values from a truncated normal and the bias with zero.\n",
    "kernels1 = tf.Variable(tf.truncated_normal(kernel_shape1, stddev=0.1, seed=10), kernel_shape1)\n",
    "biases1  = tf.Variable(tf.zeros(bias_shape1), dtype=tf.float32)\n",
    "\n",
    "# Here we are defining the convolution operation by applying kernel to the input, aka the images.\n",
    "# Stride is (1,1,1,1) and padding is SAME since we want to keep the dimensions.\n",
    "conv1 = tf.nn.conv2d(x, kernels1, strides=(1, 1, 1, 1), padding='SAME', name='conv1')\n",
    "\n",
    "# As activation function we are using a hyperbolic tangent.\n",
    "activation1 = tf.nn.tanh(conv1 + biases1, name='activation1')\n",
    "\n",
    "\n",
    "#############\n",
    "# LAYER TWO #\n",
    "#############\n",
    "\n",
    "# Now we are pooling the results with a 2x2 kernel in order to get 16 16x16 feature maps.\n",
    "pool1 = tf.nn.max_pool(activation1, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')\n",
    "\n",
    "\n",
    "###############\n",
    "# LAYER THREE #\n",
    "###############\n",
    "\n",
    "# Again we are defining the shapes of our new kernels.\n",
    "# We have 16 16x16 feature maps and we want 32 16x16 feature maps based on a 32 3x3 kernels.\n",
    "kernel_shape2 = (3, 3, 16, 32)\n",
    "bias_shape2   = (32,)\n",
    "kernels2      = tf.Variable(tf.truncated_normal(kernel_shape2, stddev=0.1, seed=10), kernel_shape2)\n",
    "biases2       = tf.Variable(tf.zeros(bias_shape2), dtype=tf.float32)\n",
    "\n",
    "# Define the next convolution operation with the previous output pool1 and the new kernels kernels2.\n",
    "conv2         = tf.nn.conv2d(pool1, kernels2, strides=(1, 1, 1, 1), padding='SAME', name='conv2')\n",
    "activation2   = tf.nn.tanh(conv2 + biases2, name='activation2')\n",
    "\n",
    "\n",
    "##############\n",
    "# LAYER FOUR #\n",
    "##############\n",
    "\n",
    "# Again we are pooling the results with a 2x2 kernel in order to get 32 8x8 feature maps.\n",
    "pool2 = tf.nn.max_pool(activation2, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')\n",
    "\n",
    "\n",
    "##############\n",
    "# LAYER FIVE #\n",
    "##############\n",
    "\n",
    "# Now we want to direct the output into a FFNN. For that reason we have to reshape the data into\n",
    "# a vector format.\n",
    "pool2_reshaped = tf.reshape(pool2, (-1, 2048), name='reshaped1')\n",
    "\n",
    "# There are 512 neurons that are connected to 8*8*32=2048 kernel pixels.\n",
    "fc_weights1 = tf.get_variable(\n",
    "    'fully-connected-1-weights',\n",
    "    initializer=tf.truncated_normal_initializer(),\n",
    "    shape=(8 * 8 * 32, 512),\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Each neuron has a bias associated.\n",
    "fc_bias1 = tf.get_variable(\n",
    "    'fully-connected-1-bias', \n",
    "    initializer=tf.zeros_initializer(), \n",
    "    shape=(512,)\n",
    ")\n",
    "\n",
    "# The activation function.\n",
    "fc1 = tf.nn.tanh(tf.matmul(pool2_reshaped, fc_weights1) + fc_bias1)\n",
    "\n",
    "\n",
    "#############\n",
    "# LAYER SIX #\n",
    "#############\n",
    "\n",
    "fc_weights2 = tf.get_variable(\n",
    "        'fully-connected-2-weights',\n",
    "        initializer=tf.truncated_normal_initializer(),\n",
    "        shape=(512, 10),\n",
    "        dtype=tf.float32\n",
    "      )\n",
    "fc_bias2 = tf.get_variable(\n",
    "    'fully-connected-2-bias', \n",
    "    initializer=tf.zeros_initializer(), \n",
    "    shape=(10,)\n",
    ")\n",
    "\n",
    "# Now we compute the drive of each neuron.\n",
    "fc2_logit = tf.matmul(fc1, fc_weights2) + fc_bias2\n",
    "\n",
    "# for the last layer we are choosing a softmax activation function and, like in previous\n",
    "# homeworks, reduce the cross entropy during training.\n",
    "cross_entropy      = tf.nn.softmax_cross_entropy_with_logits(logits=fc2_logit, labels=l_one_hot)\n",
    "mean_cross_entropy = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Here we are defining the optimizer and the accuracy and actually train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training parameters.\n",
    "batch_size    = 100\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Here we are using the AdamOptimizer as it yields better results in our task.\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(mean_cross_entropy)\n",
    "\n",
    "# Check whether the strongest firing neuron coincides with the max value position in the real labels.\n",
    "correct_prediction = tf.equal(tf.argmax(fc2_logit, 1), tf.argmax(l_one_hot, 1))\n",
    "\n",
    "# The accuracy is the mean of the matching results with the respective labels.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "for index, (data, labels) in enumerate(cifar.get_training_batch(batch_size)):\n",
    "    print(labels.shape)\n",
    "return\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        print('Starting epoch %d' % epoch)\n",
    "        for index, (data, labels) in enumerate(cifar.get_training_batch(batch_size)):\n",
    "            sess.run(\n",
    "                [train_step],\n",
    "                feed_dict={x: data, l: labels[:, np.newaxis]}\n",
    "            )\n",
    "            if index % 20 == 0:\n",
    "                test_acc = sess.run([accuracy], feed_dict={x:\n",
    "                    cifar._training_data[:1000], l: cifar._training_labels[:,\n",
    "                        np.newaxis][:1000]})\n",
    "                print('Current train accuracy %f' % test_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}