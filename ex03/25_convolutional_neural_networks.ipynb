{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 3\n",
    "## Exercise 1 + 2\n",
    "### Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar_helper import CIFAR # note we renamed the helper file so it doesn't syntax error\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a CIFAR data loader\n",
    "We are using the one that was provided. We had to import `numpy` there because it was missing as a dependency. We've also removed the seeding statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class CIFAR():\n",
    "    # make sure you have the data in this directory or pass another\n",
    "    def __init__(self, directory=\"./data\"):\n",
    "        self._directory = directory\n",
    "\n",
    "        self._training_data = []\n",
    "        self._training_labels = []\n",
    "        self._test_data = []\n",
    "        self._test_labels = []\n",
    "\n",
    "        self._load_traing_data()\n",
    "        self._load_test_data()\n",
    "\n",
    "        samples_n = self._training_labels.shape[0]\n",
    "        random_indices = np.random.choice(samples_n, samples_n // 10, replace = False)\n",
    "\n",
    "        self._validation_data = self._training_data[random_indices]\n",
    "        self._validation_labels = self._training_labels[random_indices]\n",
    "        self._training_data = np.delete(self._training_data, random_indices, axis = 0)\n",
    "        self._training_labels = np.delete(self._training_labels, random_indices)\n",
    "\n",
    "\n",
    "    def _load_traing_data(self):\n",
    "        for i in range(1, 6):\n",
    "            path = os.path.join(self._directory, \"data_batch_\" + str(i))\n",
    "            with open(path, 'rb') as fd:\n",
    "                cifar_data = pickle.load(fd, encoding = \"bytes\")\n",
    "                imgs = cifar_data[b\"data\"].reshape([-1, 3, 32, 32])\n",
    "                imgs = imgs.transpose([0, 2, 3, 1])\n",
    "                if i == 1:\n",
    "                    self._training_data = imgs\n",
    "                    self._training_labels = cifar_data[b\"labels\"]\n",
    "                else:\n",
    "                    self._training_data = np.concatenate([self._training_data, imgs], axis = 0)\n",
    "                    self._training_labels = np.concatenate([self._training_labels, cifar_data[b\"labels\"]])\n",
    "\n",
    "    def _load_test_data(self):\n",
    "        path = os.path.join(self._directory, \"test_batch\")\n",
    "        with open(path, 'rb') as fd:\n",
    "            cifar_data = pickle.load(fd, encoding = \"bytes\")\n",
    "            imgs = cifar_data[b\"data\"].reshape([-1, 3, 32, 32])\n",
    "            self._test_data = imgs.transpose([0, 2, 3, 1])\n",
    "            self._test_labels = np.array(cifar_data[b\"labels\"])\n",
    "\n",
    "    def get_training_batch(self, batch_size):\n",
    "        return self._get_batch(self._training_data, self._training_labels, batch_size)\n",
    "\n",
    "    def get_validation_batch(self, batch_size):\n",
    "        return self._get_batch(self._validation_data, self._validation_labels, batch_size)\n",
    "\n",
    "    def get_test_batch(self, batch_size):\n",
    "        return self._get_batch(self._test_data, self._test_labels, batch_size)\n",
    "\n",
    "    def _get_batch(self, data, labels, batch_size):\n",
    "        samples_n = labels.shape[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = samples_n\n",
    "\n",
    "        random_indices = np.random.choice(samples_n, samples_n, replace = False)\n",
    "        data = data[random_indices]\n",
    "        labels = labels[random_indices]\n",
    "        for i in range(samples_n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "            yield data[on:off], labels[on:off]\n",
    "\n",
    "\n",
    "    def get_sizes(self):\n",
    "        training_samples_n = self._training_labels.shape[0]\n",
    "        validation_samples_n = self._validation_labels.shape[0]\n",
    "        test_samples_n = self._test_labels.shape[0]\n",
    "        return training_samples_n, validation_samples_n, test_samples_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = CIFAR(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3 - Investigating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data to inspect.\n",
    "test_batch = list(zip(cifar._training_data[:16], cifar._training_labels[:16]))\n",
    "\n",
    "# Define the categories that correspond to the numeric labels.\n",
    "categories = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse',\n",
    "        'Ship', 'Truck']\n",
    "f, axarr = plt.subplots(4, 4)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        index = 4 * i + j\n",
    "        ax = axarr[i][j]\n",
    "        img = test_batch[index][0]\n",
    "        label = test_batch[index][1]\n",
    "        ax.set_title(categories[label])\n",
    "        ax.imshow(img)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "How many neurons are simulated? How many weights (DoF) does the network hav? How many floating point operations are necessary for a forward pass of the network?\n",
    "\n",
    "### Layer One: Conv Layer\n",
    "    \n",
    "    24*24 = 576 Neurons\n",
    "\n",
    "    16 Kernels of size (5,5,3)\n",
    "    5*5*3*16 = 9216 weights\n",
    "    \n",
    "    576 * 9216 = 5308416 floating point operations\n",
    "    \n",
    "\n",
    "### Layer Two: Max Pooling\n",
    "    \n",
    "    24*24 -> 12*12=144 neurons\n",
    "\n",
    "    no weights\n",
    "    \n",
    "    24*24 = 576 floating point operations\n",
    "\n",
    "### Layer Three: Conv Layer\n",
    "\n",
    "    144 Neurons\n",
    "    \n",
    "    32 Kernels with size (3,3,16)\n",
    "    32*16*3*3 = 4608 weights\n",
    "    \n",
    "    144 * 4608 = 663552 floating point operations\n",
    "\n",
    "### Layer Four: Pooling\n",
    "    \n",
    "    12*12 -> 6*6 = 36 neurons\n",
    "    \n",
    "    no weights\n",
    "    \n",
    "    12*12 = 144 floating point operations\n",
    "    \n",
    "### Layer Five: fully connected\n",
    "\n",
    "    2048 neurons\n",
    "    \n",
    "    36*2048 = 73728 weights\n",
    "    \n",
    "    36*(2048+2047) + 2048 = 147420 floating point operations\n",
    "\n",
    "### Layer Six: fully connected\n",
    "\n",
    "    512 neurons\n",
    "    \n",
    "    2048*512 = 1048576 weights\n",
    "    \n",
    "    2048*(512+511) + 512 = 2095616 floating point operations\n",
    "\n",
    "### Output: \n",
    "\n",
    "    10 neurons\n",
    "    \n",
    "    512*10 = 5120 weights\n",
    "    \n",
    "    512*(10+9) + 10 = 9738 floating point operations\n",
    "\n",
    "### Total:\n",
    "\n",
    "    Number of neurons: (576 + 144 + 144 + 36) + (2048 + 512 + 10) = 3470.\n",
    "    \n",
    "    Number of weights (DoF): (9216 + 0 + 4608 + 0) + (73728 + 1048576 + 5120) = 1141248.\n",
    "    \n",
    "    Floating point operations: (5308416 + 576 + 663552 + 144) + (147420 + 2095616 + 9738) = 8225462.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "In this section, we implement the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define helper functions for layer creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter for autmatically creating conv layer variable names\n",
    "conv_n = 0\n",
    "\n",
    "def conv_layer(input, kshape, strides=(1, 1, 1, 1)):\n",
    "    '''Create a convolutional layer with fixed activation function and variable\n",
    "    initialisation. The activation function is ``tf.nn.tanh`` and variables are\n",
    "    initialised from a truncated normal distribution with an stddev of 0.1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input   :   tf.Variable\n",
    "                Input to the layer\n",
    "    kshape  :   tuple or list\n",
    "                Shape of the kernel tensor\n",
    "    strides :   tuple or list\n",
    "                Strides\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "            The variable representing the layer activation (tanh(conv + bias))\n",
    "\n",
    "    '''\n",
    "    global conv_n\n",
    "    conv_n += 1\n",
    "    # this adds a prefix to all variable names\n",
    "    with tf.variable_scope('conv%d' % conv_n):\n",
    "        # We initialize bias + kernel with values from a truncated normal\n",
    "        kernels = tf.Variable(tf.truncated_normal(kshape, stddev=0.1, seed=SEED),\n",
    "                kshape)\n",
    "        bias_shape = (kshape[-1],)\n",
    "        biases = tf.Variable(tf.truncated_normal(bias_shape, stddev=0.1, seed=SEED))\n",
    "        # Here we are defining the convolution operation by applying kernel to the input, aka the images.\n",
    "        # Stride is (1,1,1,1) and padding is SAME since we want to keep the dimensions.\n",
    "        conv = tf.nn.conv2d(input, kernels, strides, padding='SAME', name='conv')\n",
    "        activation = tf.nn.tanh(conv + biases, name='activation')\n",
    "        return activation\n",
    "\n",
    "  # counter for autmatically creating fully-connected layer variable names\n",
    "fc_n = 0\n",
    "def fully_connected(input, n_out, with_activation=False):\n",
    "    '''Create a fully connected layer with fixed activation function and variable\n",
    "    initialisation. The activation function is ``tf.nn.tanh`` and variables are\n",
    "    initialised from a truncated normal distribution with an stddev of 0.1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input   :   tf.Variable\n",
    "                Input to the layer\n",
    "    n_out   :   int\n",
    "                Number of neurons in the layer\n",
    "    with_activation :   bool\n",
    "                        Return activation or drive (useful when planning to use\n",
    "                        ``softmax_cross_entropy_with_logits`` which requires\n",
    "                        unscaled logits)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Variable\n",
    "            The variable representing the layer activation (tanh(input * Weights\n",
    "            + bias))\n",
    "    '''\n",
    "    global fc_n\n",
    "    fc_n += 1\n",
    "    with tf.variable_scope('fully%d' % fc_n):\n",
    "        init = tf.truncated_normal_initializer(stddev=0.1, seed=SEED)\n",
    "        W = tf.get_variable(\n",
    "                'weights',\n",
    "                initializer=init,\n",
    "                shape=(input.shape[-1], n_out),     # the last dim of the input\n",
    "               dtype=tf.float32                     # is the first dim of the weights2\n",
    "            )\n",
    "        bias = tf.get_variable('bias', initializer=init, shape=(n_out,))\n",
    "        drive = tf.matmul(input, W) + bias\n",
    "        if with_activation:\n",
    "            return tf.nn.tanh(drive)\n",
    "        else:\n",
    "            return drive\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "The `train` function trains the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size=500, learning_rate=1e-4, epochs=10, record_step=20):\n",
    "    '''Train the fixed graph on CIFAR-10.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size  :   int\n",
    "                    Size of training batch\n",
    "    learning_rate   :   float\n",
    "                        Learning rate for the ADAM optimizer\n",
    "    epochs          :   int\n",
    "                        Number of times to visit the entire training set\n",
    "    record_step     :   int\n",
    "                        Accuracy on test set will be recorded every\n",
    "                        ``record_step`` training steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "            Array of cross entropies and array of test accuracies in a tuple\n",
    "    '''\n",
    "\n",
    "    assert batch_size > 0, 'Batch size must be positive'\n",
    "    assert learning_rate > 0, 'Learning rate must be positive'\n",
    "    assert epochs > 0, 'Number of epochs must be positive'\n",
    "    assert record_step > 0, 'Recording step must be positive'\n",
    "\n",
    "    # Reset graph in order to cope with multiple cell executions.\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Define placeholders for both the input x and the labels l.\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input')\n",
    "    l = tf.placeholder(dtype=tf.uint8, shape=(None, 1), name='labels')\n",
    "    # The labels are encoded as one-hot vectors.\n",
    "    l_one_hot = tf.squeeze(tf.one_hot(l, 10), axis=1)\n",
    "\n",
    "\n",
    "    #############\n",
    "    # LAYER ONE #\n",
    "    #############\n",
    "    # There are 16 Kernels with a shape of 5x5x3 each, plus a bias.\n",
    "    kernel_shape1 = (5, 5, 3, 16)\n",
    "    activation1 = conv_layer(x, kernel_shape1)\n",
    "    \n",
    "    #############\n",
    "    # LAYER TWO #\n",
    "    #############\n",
    "    # Now we are pooling the results with a 2x2 kernel in order to get 16 16x16 feature maps.\n",
    "    pool1 = tf.nn.max_pool(activation1, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n",
    "            padding='SAME')\n",
    "\n",
    "    kernel_shape2 = (3, 3, 16, 32)\n",
    "    activation2 = conv_layer(pool1, kernel_shape2)\n",
    "\n",
    "    ###############\n",
    "    # LAYER THREE #\n",
    "    ###############\n",
    "    # Again we are pooling the results with a 2x2 kernel in order to get 32 8x8 feature maps.\n",
    "    pool2 = tf.nn.max_pool(activation2, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')\n",
    "\n",
    "    ##############\n",
    "    # LAYER FIVE #\n",
    "    ##############\n",
    "\n",
    "    # Now we want to direct the output into a FFNN. For that reason we have to reshape the data into\n",
    "    # a vector format.\n",
    "    pool2_reshaped = tf.reshape(pool2, (-1, 2048), name='reshaped1')\n",
    "\n",
    "    # There are 512 neurons that are connected to 8*8*32=2048 kernel pixels.\n",
    "    fc1 = fully_connected(pool2_reshaped, 512, with_activation=True)\n",
    "\n",
    "    #############\n",
    "    # LAYER SIX #\n",
    "    #############\n",
    "\n",
    "    fc2_logit = fully_connected(fc1, 10)\n",
    "\n",
    "    # for the last layer we are choosing a softmax activation function and, like in previous\n",
    "    # homeworks, reduce the cross entropy during training.\n",
    "    cross_entropy      = tf.nn.softmax_cross_entropy_with_logits(logits=fc2_logit, labels=l_one_hot)\n",
    "    mean_cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(mean_cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(fc2_logit, 1), tf.argmax(l_one_hot, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    \n",
    "    cifar = CIFAR('data')\n",
    "    ###############################################\n",
    "    #  Pointless array preallocation for records  #\n",
    "    ###############################################\n",
    "    N, _, _= cifar.get_sizes()\n",
    "    n_propagations = (N // batch_size)\n",
    "    if N % batch_size != 0:\n",
    "        n_propagations += 1\n",
    "    n_entropies = n_propagations * epochs\n",
    "    entropies = np.zeros(n_entropies, dtype=np.float32)\n",
    "    n_accuracies = n_entropies // record_step\n",
    "    if n_entropies % record_step != 0:\n",
    "        n_accuracies += 1\n",
    "    accuracies = np.zeros(n_accuracies, dtype=np.float32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        propagation = 0\n",
    "        accu_counter = 0\n",
    "        for epoch in range(epochs):\n",
    "            print('Starting epoch %d' % epoch)\n",
    "            for data, labels in cifar.get_training_batch(batch_size):\n",
    "                entropy, _ = sess.run([mean_cross_entropy, train_step],\n",
    "                        feed_dict={x: data, l: labels[:, np.newaxis]})\n",
    "                entropies[propagation] = entropy\n",
    "                if propagation % record_step == 0:\n",
    "                    test_acc = sess.run([accuracy], feed_dict={x:\n",
    "                        cifar._test_data, l: cifar._test_labels[:,\n",
    "                            np.newaxis]})\n",
    "                    accuracies[accu_counter] = test_acc[0]\n",
    "                    accu_counter += 1\n",
    "                    print('Current test accuracy %f' % test_acc[0])\n",
    "                propagation += 1\n",
    "    return entropies, accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we purposefully omitted plotting entropies/accuracies\n",
    "# for the training set, as that isn't very interesting to us\n",
    "batch_size = 512\n",
    "epochs = 3\n",
    "entropies, accuracies = train(\n",
    "    batch_size=batch_size, epochs=epochs)\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(121)\n",
    "ax.set_title('Mean entropy over batch (size %d)' % batch_size)\n",
    "ax.set_xlabel('Training step')\n",
    "ax.set_ylabel('Entropy')\n",
    "                                                                          \n",
    "ax2 = f.add_subplot(122)\n",
    "ax2.set_title('Accuracy on the test set')\n",
    "ax2.set_xlabel('Training step')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "                                                                          \n",
    "ax.plot(entropies)\n",
    "ax2.plot(np.linspace(0, len(entropies), num=len(accuracies)), accuracies)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations\n",
    "\n",
    "We tried the following angles:\n",
    "* We tried all optimizers which TF provides\n",
    "* Learning rates from 0.1 to 0.0001\n",
    "* Batch sizes from 50 to 4000\n",
    "* We added a dropout to layers 1 and 2, but as far as I understand it, that reduces overfitting, but doesn't improve network potency as such.\n",
    "* We added another conv layer\n",
    "* We tried ReLU activations\n",
    "\n",
    "We trained for 35 epochs. The result was that we never reached much more than 60% accuracy. The winner on the original network was RMSProp with learning rate 0.001 and batch size 500. We assume the network is simply not capable of much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}