{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The backpropagation algorithm\n",
    "\n",
    "1. $\\hat{y} = \\sigma(\\sigma(\\sigma(\\bar{x}^T W_1)W_2)\\bar{w}_3)$ with the input vector $\\bar{x}$ and the weight matrix $W$\n",
    "\n",
    "2. $loss = \\sum_i \\frac{1}{2} (\\hat{y}_i - y_i)^2$ with the output of the network-function $\\hat{y}$ and the target value $y$\n",
    "\n",
    "3. $\\nabla_{w_{out}}loss = \\sum_j (y_i - \\hat{y}_i) \\cdot (-\\sigma^\\prime(\\bar{a}_2 \\bar{w_3})) \\cdot \\bar{a}_2$ with the network output $\\hat{y}$, the target value $y$, the derivative of the activation function $\\sigma^\\prime$, the activation of the second hidden network-layer $\\bar{a}_2 = \\sigma(\\bar{x}^T W_2)$ and the output weights $\\bar{w}_3$\n",
    "\n",
    "4. $\\sigma^\\prime(x) = \\frac{0 \\cdot (1 + e^{-x}) - 1 \\cdot (-e^{-x})}{(1 + e^{-x})^2} = \\frac{e^{-x}}{(1 + e^{-x})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats and dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 30\n",
    "\n",
    "# create training data\n",
    "np.random.seed(1)\n",
    "cats = np.random.normal(25, 5, (2, sample_size))\n",
    "dogs = np.random.normal(45, 15, (2, sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cats[0], cats[1], 'bo', dogs[0], dogs[1], 'ro')\n",
    "plt.legend(['Cats', 'Dogs'])\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Height')\n",
    "plt.title('Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(np.hstack((cats, dogs)), axis=1)\n",
    "std = np.std(np.hstack((cats, dogs)), axis=1)\n",
    "\n",
    "# normalize data\n",
    "cats = ((cats.T - mean) / std).T\n",
    "dogs = ((dogs.T - mean) / std).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cats[0], cats[1], 'bo', dogs[0], dogs[1], 'ro')\n",
    "plt.legend(['Cats', 'Dogs'])\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Height')\n",
    "plt.title('Normalized Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic activation function\n",
    "def activation(x, derive=False):\n",
    "    if derive:\n",
    "        return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "cats = np.vstack((cats, -np.ones(sample_size)))\n",
    "dogs = np.vstack((dogs, np.ones(sample_size)))\n",
    "data = np.hstack((cats, dogs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict output value y\n",
    "def predict(x, w):\n",
    "    return activation(np.dot(x.T, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square loss function\n",
    "def loss(y, y_hat):\n",
    "    return np.sum(0.5 * (y_hat - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc gradient of the loss\n",
    "def backprop(x, y, y_hat, w):\n",
    "    return np.sum((y_hat - y) * activation(np.dot(x.T, w), True) * x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weights\n",
    "weights = np.array([-2.5, -2.5])\n",
    "\n",
    "plt_w = np.zeros((epochs, 2))\n",
    "plt_el = np.zeros(epochs)\n",
    "\n",
    "# batch gradient descent\n",
    "for training_step in range(epochs):\n",
    "    y_hat = predict(data[:2], weights)\n",
    "    el = loss(data[2:], y_hat)\n",
    "    weights -= learning_rate * backprop(data[:2], data[2:], y_hat, weights)\n",
    "    plt_w[training_step] = weights\n",
    "    plt_el[training_step] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weights\n",
    "weights = np.array([-2.5, -2.5])\n",
    "\n",
    "plt_w = np.zeros((epochs, 2))\n",
    "plt_el = np.zeros(epochs)\n",
    "\n",
    "# stochastic gradient descent\n",
    "for training_step in range(60):\n",
    "    np.random.shuffle(data.T)\n",
    "    el = 0.0\n",
    "    for i in range(len(data[0])):\n",
    "        y_hat = predict(data[:2, i:i+1], weights)\n",
    "        el += loss(data[2:, i:i+1], y_hat)\n",
    "        weights -= learning_rate * backprop(data[:2, i:i+1], data[2:, i:i+1], y_hat, weights)\n",
    "    plt_w[training_step] = weights\n",
    "    plt_el[training_step] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "# init weights\n",
    "weights = np.array([-2.5, -2.5])\n",
    "\n",
    "plt_w = np.zeros((epochs, 2))\n",
    "plt_el = np.zeros(epochs)\n",
    "\n",
    "# mini-batch gradient descent\n",
    "for training_step in range(60):\n",
    "    np.random.shuffle(data.T)\n",
    "    el = 0.0\n",
    "    for i in range(0, len(data[0]), batch_size):\n",
    "        y_hat = predict(data[:2, i:i+batch_size], weights)\n",
    "        el += loss(data[2:, i:i+batch_size], y_hat)\n",
    "        weights -= learning_rate * backprop(data[:2, i:i+batch_size], data[2:, i:i+batch_size], y_hat, weights)\n",
    "    plt_w[training_step] = weights\n",
    "    plt_el[training_step] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot error surface\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-5, 30, 40)\n",
    "y = np.linspace(-5, 30, 40)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros(X.shape)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i,j] = loss(data[2:], predict(data[:2], np.array([X[i,j], Y[i,j]])))\n",
    "\n",
    "cp = ax.contourf(X, Y, Z, cmap='viridis')\n",
    "ax.contour(X, Y, Z, colors='black', linestyles='dashed')\n",
    "ax.plot(plt_w[:,0], plt_w[:,1], 'w')\n",
    "fig.colorbar(cp)\n",
    "\n",
    "# plot square loss\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(epochs), plt_el)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weights\n",
    "weights = np.array([-2.5, -2.5])\n",
    "delta_w = np.zeros(2)\n",
    "\n",
    "momentum = 0.9\n",
    "\n",
    "plt_w = np.zeros((epochs, 2))\n",
    "plt_el = np.zeros(epochs)\n",
    "\n",
    "# batch gradient descent with momentum\n",
    "for training_step in range(epochs):\n",
    "    y_hat = predict(data[:2], weights)\n",
    "    el = loss(data[2:], y_hat)\n",
    "    delta_w = momentum * delta_w + learning_rate * backprop(data[:2], data[2:], y_hat, weights)\n",
    "    weights -= delta_w\n",
    "    plt_w[training_step] = weights\n",
    "    plt_el[training_step] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
