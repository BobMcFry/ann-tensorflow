{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### Part 1\n",
    "The network-function of the given neural network is denoted as follows:\n",
    "$$y = \\sigma(\\overline{w}_3\\sigma(W_2\\sigma(W_1\\overline{x})))$$\n",
    "where $\\overline{x}$ is the input vector, $W_i$ is the weight matrix for layer with indexes $i = \\{1, 2\\}$ and $\\overline{w}_3$ is the weight vector for the last neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 2\n",
    "The sum squared error function looks like the following:\n",
    "$$E(y) = \\sum_i \\frac{1}{2}(y_i - \\hat{y}_i)^2 \\text{.}$$\n",
    "The variable $y$ represents our estimated output and the variable $\\hat{y}$ denotes the actual value. Inserting our formula from part $1$ yields\n",
    "$$\n",
    "E(X) = \\sum_{\\overline{x}_i\\in X} \\frac{1} {2}(\\sigma(\\overline{w}_3\\sigma(W_2\\sigma(W_1\\overline{x}_i))) - \\hat{y}_i)^2$$\n",
    "where $X$ denotes our data set and $x_i$ is the $i$-th input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "$$\\newcommand{\\ov}[1]{\\overline{#1}}$$\n",
    "Let $$K(x) =  \\sigma(W_2\\sigma(W_1\\overline{x}_i))$$ be all the parts independent of $w_3$. Thus\n",
    "$$\n",
    "\\begin{align}\n",
    "E(X)  = & \\sum_{\\overline{x}_i \\in X}\\frac{1}{2}\\cdot(\\sigma(w_3 \\cdot K(\\overline{x}_i)) - \\hat{y}_i)^2\\\\ \\text{and the gradient}\\\\\n",
    "\\nabla_{w_3} E(X) = & \\sum_{\\ov{x}_i} (\\sigma(w_3 \\cdot K(\\ov{x}_i)) - \\hat{y}_i) \\cdot  (\\nabla_{w_3}\\sigma(w_3K(\\ov{x}_i)))\\\\\n",
    "\\nabla_{w_3} E(X) = & \\sum_{\\ov{x}_i} (\\sigma(w_3 \\cdot K(\\ov{x}_i)) - \\hat{y}_i) \\cdot  (K(\\ov{x}_i) \\cdot \\sigma^\\prime(w_3 \\cdot K(\\ov{x}_i)))\\\\\n",
    "\\nabla_{w_3} E(X) = & \\sum_{\\ov{x}_i} (\\sigma(w_3 \\cdot K(\\ov{x}_i)) - \\hat{y}_i)  \\cdot \\sigma^\\prime(w_3 \\cdot K(\\ov{x}_i)) \\cdot K(\\ov{x}_i)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "Given is the logistic function\n",
    "$$\\sigma(x) = \\frac{1}{1+\\mathcal{e}^{-x}} = (1 + \\mathcal{e}^x)^{-1}.$$\n",
    "Now we derive that function with respect to $x$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^\\prime(x) &= -(1 + \\mathcal{e}^{x})^{-2} \\cdot (-\\mathcal{e}^{x}) &\\text{// chain rule}\\\\\n",
    "&= \\frac{\\mathcal{e}^{x}}{(1 + \\mathcal{e}^{x})^2}\\\\\n",
    "&= \\frac{\\mathcal{e}^{x}}{1 + 2\\mathcal{e}^{x} + \\mathcal{e}^{2x}}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex 3\n",
    "# Copyright 2017 Rasmus\n",
    "\n",
    "# Licensed under the \"THE BEER-WARE LICENSE\" (Revision 42):\n",
    "# Rasmus wrote this file. As long as you retain this notice you\n",
    "# can do whatever you want with this stuff. If we meet some day, and you think\n",
    "# this stuff is worth it, you can buy me a beer or coffee in return\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import normal\n",
    "from scipy.special import expit as logistic\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(1)\n",
    "\n",
    "sample_size = 60\n",
    "\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3, 4, 5, 6, 7, 8, 9 + Exercise 5\n",
    "\n",
    "Note: It is recommended to peruse the `.py` file uploaded as well, since I have not adapted the code to a notebook format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to the reader: Exercise sheet specifies to use a variance of V in the\n",
    "# text, but then uses an std of V in the code\n",
    "def dog_distribution(shape):\n",
    "    '''Sample from normal pdf with \u00b5=45 AND \u03c3=15\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape   :   tuple\n",
    "                Shape of array to fill\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "            Array of specified shape sampled from distribution of\n",
    "    '''\n",
    "    return normal(45, 15, shape)\n",
    "\n",
    "\n",
    "def cat_distribution(shape):\n",
    "    '''Sample from normal pdf with \u00b5=25 AND \u03c3=5\n",
    "    Parameters\n",
    "    ----------\n",
    "    shape   :   tuple\n",
    "                Shape of array to fill\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "            Array of specified shape sampled from distribution of\n",
    "    '''\n",
    "    return normal(25, 5, shape)\n",
    "\n",
    "\n",
    "def logistic_derivative(batch):\n",
    "    '''Derivative of the logistic (expit) function, derived by hand\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch   :   np.ndarray of dimension 1\n",
    "                Values to pump through this function\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "    '''\n",
    "    # f(x) = e^x / (1+e^x)^2\n",
    "    # this can operate on entire arrays elementwise\n",
    "    e = np.exp(batch)\n",
    "    return e / np.square(1 + e)\n",
    "\n",
    "\n",
    "def forward_pass(W, batch):\n",
    "    '''Propagate data through the network given by W.\n",
    "    Parameters\n",
    "    ----------\n",
    "    W   :   np.ndarray\n",
    "            Array of shape (1, 2) giving the network weights\n",
    "    batch   :   np.ndarray\n",
    "                Array of shape (N, 2) with the data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "            Network predictions for each sample in shape (N, 1)\n",
    "    '''\n",
    "    # sanity check our shapes\n",
    "    assert batch.shape[1] == 2, 'Data shape is incorrect'\n",
    "    assert W.shape == (1, 2), 'Weights shape is incorrect'\n",
    "    return logistic(np.dot(W, batch.T)).T\n",
    "\n",
    "\n",
    "def loss_function(predictions, targets):\n",
    "    '''Calculate sum of squared error loss.\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions     :   np.ndarray\n",
    "                        Array of values from [0,1] (the model output)\n",
    "    targets     :   np.ndarray\n",
    "                    Target values, of same shape and dtype as `predictions`\n",
    "    Returns\n",
    "    -------\n",
    "    double\n",
    "        SSE-Loss, summed over all rows\n",
    "\n",
    "    '''\n",
    "    return 0.5 * np.sum(np.square(predictions - targets))\n",
    "\n",
    "\n",
    "def gradient_loss(W, batch, targets):\n",
    "    '''Compute the gradient of the loss function with respect to W.\n",
    "    Parameters\n",
    "    ----------\n",
    "    W   :   np.ndarray\n",
    "            Weight matrix of shape (1, 2)\n",
    "    batch   :   np.ndarray\n",
    "                Data to compute the loss on, of shape (N, 2)\n",
    "    targets :   np.ndarray\n",
    "                Targets of shape (N, 1)\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "            Gradient vector of shape (1, 2), averaged accross the data set\n",
    "    '''\n",
    "    assert batch.shape[1] == 2, 'Data shape is incorrect'\n",
    "    # we want to take the scalar product of the weights with each data row, so\n",
    "    # we must transpose the data to get (1, 2) x (2, N) => (1, N)\n",
    "    e = np.exp(-np.dot(W, batch.T))\n",
    "    # refer to the lecture slides, gradient for only one layer consists of only\n",
    "    # 3 terms. More layers lead to more chain rules and more terms\n",
    "    term_1 = forward_pass(W, batch) - targets   # shape (N, 1)\n",
    "    term_2 = - logistic_derivative(batch)       # shape (N, 1)\n",
    "    term_3 = batch                              # shape (N, 2)\n",
    "    grad = term_1 * term_2 * term_3             # term_3 is broadcast (stacked)\n",
    "    return np.mean(grad, axis=0)\n",
    "\n",
    "\n",
    "def batches(data, targets, size):\n",
    "    '''Generate minibatches from data and targets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data    :   np.ndarray\n",
    "                Array of shape (N, 2)\n",
    "    targets :   np.ndarray\n",
    "                Array of classes of shape (N, 1)\n",
    "    size    :   int\n",
    "                Batch size. Must be >= 1\n",
    "    Yields\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple of (data_batch, target_batch) subarrays\n",
    "    '''\n",
    "    assert size >= 1, 'Batch size must be positive'\n",
    "    assert data.shape[1] == 2, 'Data shape is incorrect'\n",
    "    data_len = data.shape[0]\n",
    "    # make one array of data + targets to shuffle at the same time\n",
    "    data_targets = np.concatenate((data, targets), axis=1)\n",
    "    np.random.shuffle(data_targets)\n",
    "    data = data_targets[:, :2]  # retrieve shuffled data\n",
    "    targets = data_targets[:, 2:]   # retrieve shuffled targets and keep (N, 1)\n",
    "    # shape instead of 1d shape\n",
    "    # we go accross the rows in leaps\n",
    "    for i in range(0, data_len // size):\n",
    "        start = i * size\n",
    "        end = min(start + size, data_len)   # len may not be evenly divided\n",
    "        # yield tuple taken from the original array\n",
    "        yield data[start:end, :], targets[start:end, :]\n",
    "\n",
    "\n",
    "def train(data, targets, epochs=1000, learning_rate=0.1, bs=10,\n",
    "          W=np.array([[-2.5, -2.5]]), momentum=0.5):\n",
    "    '''Train the network.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data    :   np.ndarray\n",
    "                Array of shape (N, 2)\n",
    "    targets :   np.ndarray\n",
    "                Array of classes of shape (N, 1)\n",
    "    epochs  :   int\n",
    "                Number of times to go over the entire dataset in batches\n",
    "    learning_rate   :   float\n",
    "                        Backprop leraning learning rate\n",
    "    bs  :   int\n",
    "            Batch size\n",
    "    W   :   np.ndarray\n",
    "            Intitial weights of shape (1, 2)\n",
    "    momentum    :   float\n",
    "                    Momentum parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W   :   np.ndarray\n",
    "            Final weights of shape (1, 2)\n",
    "    all_Ws  :   np.ndarray\n",
    "                Array of all weight update steps, of shape (_, 2)\n",
    "\n",
    "    '''\n",
    "    # number of times the weights are updated\n",
    "    n_iter = epochs * (len(targets) // bs) + len(targets) % bs\n",
    "    # iteration number\n",
    "    i = 0\n",
    "    # record all weight updates for later plotting\n",
    "    all_Ws = np.zeros((n_iter, 2))\n",
    "\n",
    "    delta_w = np.zeros((1, 2))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # one epoch means going over the entire dataset once\n",
    "        for batch, batch_targets in batches(data, targets, bs):\n",
    "            # record current weights\n",
    "            all_Ws[i, :] = W\n",
    "            i += 1\n",
    "            gradient = gradient_loss(W, batch, batch_targets)\n",
    "            # I don't know why I need the +-sign instead of minus, there seems\n",
    "            # to be an error somewhere\n",
    "            delta_w = momentum * delta_w + learning_rate * gradient\n",
    "            W = W + delta_w\n",
    "        # loss = loss_function(forward_pass(W, data), targets)\n",
    "        # print('W={}, loss={}'.format(W, loss))\n",
    "    return W, all_Ws\n",
    "\n",
    "\n",
    "def generate_error_surf_plot(data, targets, limits_x=(-4, 4), limits_y=(-4, 4)):\n",
    "    '''Create a contour plot of the error surface for the loss function defined\n",
    "    above.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data    :   np.ndarray\n",
    "                Array of shape (N, 2)\n",
    "    targets :   np.ndarray\n",
    "                Array of classes of shape (N, 1)\n",
    "    limits_x    :   tuple\n",
    "                    Limits for the X-axis\n",
    "    limits_y    :   tuple\n",
    "                    Limits for the Y-axis\n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        Figure with one of two subplots filled in\n",
    "\n",
    "    '''\n",
    "    f = plt.figure()\n",
    "    # subplot array of (1, 2), subplot 1\n",
    "    ax = f.add_subplot(121)\n",
    "    # step size for plot\n",
    "    delta = 0.05\n",
    "    w1 = np.arange(limits_x[0], limits_x[1] + delta, delta)\n",
    "    w2 = np.arange(limits_y[0], limits_y[1] + delta, delta)\n",
    "    Z = np.zeros((len(w2), len(w1)))\n",
    "    # now we fill the value array which has for each pair of w1, w2 the loss for\n",
    "    # those weights\n",
    "    for xi in np.arange(len(w1)):\n",
    "        for yi in np.arange(len(w2)):\n",
    "            W = np.array([[w1[xi], w2[yi]]])\n",
    "            Z[yi, xi] = loss_function(forward_pass(W, data), targets)\n",
    "    cp = ax.contourf(w1, w2, Z)\n",
    "    ax.set_xlim(limits_y)\n",
    "    ax.set_ylim(limits_x)\n",
    "    f.colorbar(cp, ax=ax)\n",
    "    return f\n",
    "\n",
    "\n",
    "def main():\n",
    "    ##########################################################################\n",
    "    #                              Generate data                               #\n",
    "    ##########################################################################\n",
    "    x_cat = cat_distribution((sample_size // 2, 2))\n",
    "    x_dog = dog_distribution((sample_size // 2, 2))\n",
    "\n",
    "    ##########################################################################\n",
    "    #                              Normalize data                              #\n",
    "    ##########################################################################\n",
    "    # Shouldn't this be done per dimension?\n",
    "    mean = np.mean([x_cat, x_dog])\n",
    "    std = np.std([x_cat, x_dog])\n",
    "\n",
    "    # not equivalent ???????\n",
    "    # x_cat -=  mean\n",
    "    # x_dog -= mean\n",
    "    # x_cat /= std\n",
    "    # x_dog /= std\n",
    "    x_cat = (x_cat - mean) / std\n",
    "    x_dog = (x_dog - mean) / std\n",
    "\n",
    "    ##########################################################################\n",
    "    #                               Plot dataset                               #\n",
    "    ##########################################################################\n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "    ax.set_title('Data distribution')\n",
    "    ax.set_xlabel('Length')\n",
    "    ax.set_ylabel('Height')\n",
    "    ax.scatter(x_cat[:, 0], x_cat[:, 1], c='blue', label='cats')\n",
    "    ax.scatter(x_dog[:, 0], x_dog[:, 1], c='orange', label='dogs')\n",
    "    ax.legend()\n",
    "\n",
    "    ##########################################################################\n",
    "    #                             Generate targets                             #\n",
    "    ##########################################################################\n",
    "    data = np.concatenate((x_cat, x_dog), axis=0)\n",
    "    # 1 = CAT, 0 = DOG\n",
    "    targets_cats = np.ones((sample_size // 2, 1))\n",
    "    targets_dogs = np.zeros((sample_size // 2, 1))\n",
    "    targets = np.concatenate((targets_cats, targets_dogs), axis=0)\n",
    "\n",
    "    ##########################################################################\n",
    "    #                               Do training                                #\n",
    "    ##########################################################################\n",
    "    figure_surf = generate_error_surf_plot(data, targets)\n",
    "    final_W, all_Ws = train(data, targets, epochs=1000, W=np.array([[-1, 4]]))\n",
    "    print('Final weights and loss: {}, {}'.format(\n",
    "        final_W, loss_function(forward_pass(final_W, data), targets)))\n",
    "    ############################################################################\n",
    "    #                  Create surface plot and show progress                   #\n",
    "    ############################################################################\n",
    "    # marker for final weights\n",
    "    circle = plt.Circle(\n",
    "        (final_W[0, 0], final_W[0, 1]), 0.2, color='r', alpha=0.2)\n",
    "    plot_surf = figure_surf.gca()\n",
    "    plot_surf.set_title('Error surface and weight updates')\n",
    "    # shapes are 'Artists' in pyplot speak\n",
    "    plot_surf.add_artist(circle)\n",
    "    # dots change color from black to white\n",
    "    plot_surf.scatter(all_Ws[:, 0], all_Ws[:, 1],\n",
    "                      c=np.linspace(0.0, 1.0, all_Ws.shape[0]),\n",
    "                      cmap='gray', s=0.5)\n",
    "    ############################################################################\n",
    "    #                       Create a plot for the loss                         #\n",
    "    ############################################################################\n",
    "    plot_loss = figure_surf.add_subplot(122)\n",
    "    all_losses = np.apply_along_axis(lambda W: loss_function(\n",
    "        forward_pass(np.array([W]), data), targets), 1, all_Ws)\n",
    "    plot_loss.plot(np.arange(all_Ws.shape[0]), all_losses)\n",
    "    plot_loss.set_title('Loss across steps')\n",
    "    plot_loss.set_xlabel('iteration')\n",
    "    plot_loss.set_ylabel('loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}